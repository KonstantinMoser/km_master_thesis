{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYV6w6d6jusN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "# --token XXX --add-to-git-credential --n"
      ],
      "metadata": {
        "id": "FOKxIH8iuc6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains several modular classes designed to handle tasks like data processing, evaluation, and model interaction in a contest evaluation pipeline.\n",
        "\n",
        "1. **DriveManager**: Manages Google Drive operations in a Colab environment, such as mounting and accessing paths.\n",
        "2. **ModelLoader**: Loads and configures Hugging Face models and tokenizers, providing methods for text generation.\n",
        "3. **DataCleaner**: Provides utilities for cleaning text, calculating token sizes, and processing model outputs.\n",
        "4. **Summarizer**: Generates concise summaries of contest descriptions using a language model.\n",
        "5. **SubmissionEvaluator**: Evaluates individual submissions by preparing a prompt with few-shot examples and querying the model.\n",
        "6. **DiscriminativeEvaluator**: Provides numeric-based evaluations for submissions, categorizing them as Excellent, Average, or Poor.\n",
        "7. **ContestEvaluator**: Orchestrates the evaluation of all submissions in a contest and optionally saves the results to a CSV.\n",
        "\n",
        "Each class is modular and designed for reusability and scalability, allowing easy customization and integration with new datasets or models.\n"
      ],
      "metadata": {
        "id": "gMrEckcNz48o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from accelerate import infer_auto_device_map"
      ],
      "metadata": {
        "id": "jhjZZ5kCoQ9a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DriveManager, ModelLoader, DataCleaner\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DriveManager\n",
        "# --------------------------------------------------\n",
        "class DriveManager:\n",
        "    \"\"\"\n",
        "    Class Name: DriveManager\n",
        "\n",
        "    Purpose:\n",
        "        - Handle mounting Google Drive in a Colab environment.\n",
        "        - Provide convenient path handling if needed.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Mount/unmount Google Drive.\n",
        "        - Potentially provide standard path resolutions for loading/saving.\n",
        "\n",
        "    Example Usage:\n",
        "        drive_mgr = DriveManager()\n",
        "        drive_mgr.mount_drive()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mount_path=\"/content/drive/\"):\n",
        "        \"\"\"\n",
        "        Constructor for DriveManager.\n",
        "\n",
        "        Args:\n",
        "            mount_path (str): The path at which to mount Google Drive.\n",
        "\n",
        "        Attributes:\n",
        "            mount_path (str): Where Google Drive is mounted in Colab.\n",
        "        \"\"\"\n",
        "        self.mount_path = mount_path\n",
        "\n",
        "    def mount_drive(self, force_remount=True):\n",
        "        \"\"\"\n",
        "        Mounts Google Drive using the provided mount path.\n",
        "\n",
        "        Args:\n",
        "            force_remount (bool): Whether to force-remount if already mounted.\n",
        "        \"\"\"\n",
        "        from google.colab import drive\n",
        "        drive.mount(self.mount_path, force_remount=force_remount)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# ModelLoader\n",
        "# --------------------------------------------------\n",
        "class ModelLoader:\n",
        "    \"\"\"\n",
        "    Class Name: ModelLoader\n",
        "\n",
        "    Purpose:\n",
        "        - Load and configure a Hugging Face model and tokenizer.\n",
        "        - Provide a convenient method for text generation.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Download/load a tokenizer and model from a given model name.\n",
        "        - Handle device placement (CPU/GPU) and half-precision casting if desired.\n",
        "        - Provide a generate_text() method for inference.\n",
        "\n",
        "    Example Usage:\n",
        "        model_loader = ModelLoader(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "        model_loader.load_model_and_tokenizer()\n",
        "        response = model_loader.generate_text(prompt, max_length=200)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, device=None):\n",
        "        \"\"\"\n",
        "        Constructor for ModelLoader.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): The name or path of the model on Hugging Face.\n",
        "            device (str, optional): The device to load model onto (e.g., \"cuda\" or \"cpu\").\n",
        "                                    Defaults to \"cuda\" if available, else \"cpu\".\n",
        "\n",
        "        Attributes:\n",
        "            model_name (str): The Hugging Face model name.\n",
        "            tokenizer (AutoTokenizer): The loaded tokenizer (populated after load_model_and_tokenizer()).\n",
        "            model (AutoModelForCausalLM): The loaded model (populated after load_model_and_tokenizer()).\n",
        "            device (str): The device for model inference.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def load_model_and_tokenizer(self):\n",
        "        \"\"\"\n",
        "        Loads the tokenizer and model from Hugging Face.\n",
        "        Casts the model to half-precision if the device is GPU.\n",
        "        \"\"\"\n",
        "        print(f\"Loading model: {self.model_name} on device {self.device}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            device_map=\"auto\", # if self.device == \"cuda\" else None,\n",
        "            torch_dtype=torch.float16 # if self.device == \"cuda\" else torch.float32\n",
        "        ).to(self.device)\n",
        "\n",
        "    def generate_text(self, prompt, max_length=512, temperature=0.7, stop_token_ids=None):\n",
        "        \"\"\"\n",
        "        Generates text using the loaded model.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): The input text/prompt for generation.\n",
        "            max_length (int): Maximum number of tokens for the output.\n",
        "            temperature (float): Sampling temperature.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text from the model.\n",
        "        \"\"\"\n",
        "        if self.tokenizer is None or self.model is None:\n",
        "            raise ValueError(\"Tokenizer/Model not loaded. Call load_model_and_tokenizer() first.\")\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "        )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DataCleaner\n",
        "# --------------------------------------------------\n",
        "class DataCleaner:\n",
        "    \"\"\"\n",
        "    Class Name: DataCleaner\n",
        "\n",
        "    Purpose:\n",
        "        - Provide static methods to clean text and calculate token sizes.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Clean text by removing special characters (except dots).\n",
        "        - Calculate the number of tokens of a given text using a tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text):\n",
        "        \"\"\"\n",
        "        Clean a text field by removing unnecessary symbols (except dots),\n",
        "        handling newlines, etc.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text to clean.\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned text or an empty string if invalid/empty.\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str) or not text.strip():\n",
        "            return \"\"\n",
        "\n",
        "        # Remove weird signs except for dots\n",
        "        cleaned = re.sub(r'[^\\w\\s.]', '', text)\n",
        "        # Replace multiple newlines with space\n",
        "        cleaned = re.sub(r'\\n+', ' ', cleaned)\n",
        "        return cleaned.strip()\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_token_size(text, tokenizer):\n",
        "        \"\"\"\n",
        "        Calculate the number of tokens in a given text using the tokenizer.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text.\n",
        "            tokenizer: The tokenizer from Hugging Face.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of tokens.\n",
        "        \"\"\"\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        return tokens.shape[1]\n",
        "\n",
        "    @staticmethod\n",
        "    def strip_prompt_and_eos_token(response, prompt, tokenizer):\n",
        "        \"\"\"\n",
        "        Strips the prompt from the response and removes the EOS token if it exists.\n",
        "\n",
        "        Args:\n",
        "            response (str): The generated text response from the model.\n",
        "            prompt (str): The input prompt used for the model.\n",
        "            tokenizer: The tokenizer used by the model, providing the EOS token.\n",
        "\n",
        "        Returns:\n",
        "            str: The cleaned response text.\n",
        "        \"\"\"\n",
        "        # Strip the prompt from the response\n",
        "        response_start = response.find(prompt[-100:])\n",
        "        if response_start != -1:\n",
        "            response = response[response_start + 100:].strip()\n",
        "\n",
        "        # Check if the response ends with the EOS token and remove it\n",
        "        eos_token = tokenizer.eos_token  # e.g., \"<|eot_id|>\"\n",
        "        if response.endswith(eos_token):\n",
        "            response = response[: -len(eos_token)].strip()\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "N2KACG6UqGJA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DiscriminativeEvaluator\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DiscriminativeEvaluator\n",
        "# --------------------------------------------------\n",
        "class DiscriminativeEvaluator:\n",
        "    \"\"\"\n",
        "    Class to evaluate submissions based on quantitative features and generate textual feedback.\n",
        "    \"\"\"\n",
        "\n",
        "    CATEGORY_EXCELLENT = 3\n",
        "    CATEGORY_AVERAGE   = 2\n",
        "    CATEGORY_POOR      = 1\n",
        "\n",
        "    def __init__(self, df):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing all submissions.\n",
        "            top_percentile (float): Percentile threshold for determining Excellent category.\n",
        "            average_percentile (float): Percentile threshold for determining Average category.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.top_percentile = 0.8\n",
        "        self.average_percentile = 0.4\n",
        "\n",
        "\n",
        "        # Simple synonyms for the 3 categories to add mild variation\n",
        "        self.category_synonyms = {\n",
        "            self.CATEGORY_EXCELLENT: [\"excellent\"], # \"outstanding\"\n",
        "            self.CATEGORY_AVERAGE:   [\"average\"], # \"moderate\"\n",
        "            self.CATEGORY_POOR:      [\"poor\"] # \"weak\"\n",
        "        }\n",
        "\n",
        "    def _choose_random_synonym(self, category):\n",
        "        \"\"\"Randomly pick a synonym for the given category.\"\"\"\n",
        "        return random.choice(self.category_synonyms[category])\n",
        "\n",
        "    def evaluate_label_category(self, submission_row, same_contest_df, label):\n",
        "        \"\"\"\n",
        "        Evaluate the given label for a submission and categorize it as 'Excellent', 'Average', or 'Poor'.\n",
        "\n",
        "        Args:\n",
        "            submission_row (pd.Series): Row of the submission to evaluate.\n",
        "            same_contest_df (pd.DataFrame): Filtered DataFrame with the same contest.\n",
        "            label (str): Label to evaluate (e.g., 'submission_word_count', 'num_image').\n",
        "\n",
        "        Returns:\n",
        "            int: The category (3 for Excellent, 2 for Average, 1 for Poor).\n",
        "        \"\"\"\n",
        "        label_series = same_contest_df[label].dropna()\n",
        "        submission_value = submission_row[label]\n",
        "\n",
        "        # Handle edge case for low averages\n",
        "        if label_series.mean() < 1 and submission_value == 0:\n",
        "            return self.CATEGORY_AVERAGE\n",
        "\n",
        "        # Determine percentile rank\n",
        "        rank_percentile = label_series.rank(pct=True)[submission_row.name]\n",
        "\n",
        "        if rank_percentile >= self.top_percentile:\n",
        "            return self.CATEGORY_EXCELLENT\n",
        "        elif rank_percentile >= self.average_percentile:\n",
        "            return self.CATEGORY_AVERAGE\n",
        "        else:\n",
        "            return self.CATEGORY_POOR\n",
        "\n",
        "    def calculate_percentile(self, submission_row, same_contest_df, label):\n",
        "        \"\"\"\n",
        "        Calculate the percentile of a submission's value for a given label.\n",
        "        \"\"\"\n",
        "        label_series = same_contest_df[label].dropna()\n",
        "\n",
        "        # Handle case with many zeroes\n",
        "        if label_series.mean() <= 0.5 and submission_row[label] == 0:\n",
        "            return 0.5  # Treat as average if the mean is very low\n",
        "\n",
        "        return label_series.rank(pct=True)[submission_row.name]\n",
        "\n",
        "    def discriminative_evaluation(self, submission_url):\n",
        "        \"\"\"\n",
        "        Perform a discriminative evaluation and return both textual feedback and a string classification.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (evaluation_text, disc_classification_str)\n",
        "                  where disc_classification_str ∈ {\"Excellent\", \"Average\", \"Poor\"}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            submission_row = self.df[self.df[\"submission_url\"] == submission_url].iloc[0]\n",
        "        except IndexError:\n",
        "            raise ValueError(f\"Submission URL {submission_url} not found in the dataset.\")\n",
        "\n",
        "        # Filter for the same contest\n",
        "        contest_name = submission_row[\"contest_name\"]\n",
        "        same_contest_df = self.df[self.df[\"contest_name\"] == contest_name]\n",
        "\n",
        "        # Determine overall category based on rank\n",
        "        rank = submission_row['qs_rank']\n",
        "        num_submissions = submission_row['num_of_submissions']\n",
        "        # Higher percentiles = better rankings\n",
        "        rank_percentile = (num_submissions - rank) / num_submissions\n",
        "\n",
        "        if rank_percentile >= self.top_percentile:  # High percentile = Excellent\n",
        "            overall_category = self.CATEGORY_EXCELLENT\n",
        "        elif rank_percentile >= self.average_percentile:  # Mid percentile = Average\n",
        "            overall_category = self.CATEGORY_AVERAGE\n",
        "        else:\n",
        "            overall_category = self.CATEGORY_POOR  # Low percentile = Poor\n",
        "\n",
        "\n",
        "        # --- Paragraph 1: Description ---\n",
        "        desc_value = submission_row[\"submission_word_count\"]\n",
        "        desc_cat = self.evaluate_label_category(submission_row, same_contest_df, \"submission_word_count\")\n",
        "        desc_syn = self._choose_random_synonym(desc_cat)\n",
        "\n",
        "        if desc_value < 50:\n",
        "            description_paragraph = (\n",
        "                \"The submission's description contains minimal content, making a proper evaluation difficult. \"\n",
        "                \"It falls short of providing key details that could help understand the project effectively.\"\n",
        "            )\n",
        "            alignment_text = \"\"\n",
        "        else:\n",
        "            if desc_cat == self.CATEGORY_EXCELLENT:\n",
        "                description_paragraph = (\n",
        "                f\"The submission’s description is {desc_syn} compared to other entries. \"\n",
        "                \"It contains an extensive amount of text that thoroughly explains the project, covering all essential aspects in detail. \"\n",
        "                \"This level of textual detail exceeds that of most other submissions.\"\n",
        "                )\n",
        "                if desc_cat == overall_category:\n",
        "                    alignment_text = (\n",
        "                        \"The length and level of detail in the description reflect the overall excellent efforts of the submission, \"\n",
        "                        \"mirroring the high standards seen throughout the project.\"\n",
        "                    )\n",
        "                elif desc_cat > overall_category:\n",
        "                    alignment_text = (\n",
        "                        \"The description stands out with superior detail compared to the overall submission quality, \"\n",
        "                        \"indicating that the textual documentation may have been prioritized.\"\n",
        "                    )\n",
        "\n",
        "            elif desc_cat == self.CATEGORY_AVERAGE:\n",
        "                description_paragraph = (\n",
        "                f\"The project’s description is {desc_syn} and offers a moderate amount of text that covers the key points of the project. \"\n",
        "                \"It provides a level of detail that is comparable to the average submission.\"\n",
        "                )\n",
        "                if desc_cat == overall_category:\n",
        "                    alignment_text = (\n",
        "                        \"The description's moderate level of detail is consistent with the overall average quality of submissions, \"\n",
        "                        \"providing an amount of text that aligns with what is typically seen in similar entries.\"\n",
        "                    )\n",
        "                elif desc_cat < overall_category:\n",
        "                    alignment_text = (\n",
        "                        \"Although the overall submission is deemed excellent, the description offers only an average level of detail, \"\n",
        "                        \"suggesting that additional elaboration could have further strengthened its documentation compared to other high-quality entries.\"\n",
        "                    )\n",
        "                else:\n",
        "                    alignment_text = (\n",
        "                        \"The description stands out with more text and detail than what is generally seen in submissions with overall poor quality, \"\n",
        "                        \"indicating that extra effort was put into documenting the project relative to its peers.\"\n",
        "                    )\n",
        "            else:  # Poor\n",
        "                description_paragraph = (\n",
        "                    f\"The description is {desc_syn} compared to other entries. \"\n",
        "                    \"It contains very little text, providing only a brief overview of the project. \"\n",
        "                    \"Compared to most other submissions, it lacks the level of detail needed to fully understand the project’s scope and functionality.\"\n",
        "                )\n",
        "                if desc_cat == overall_category:\n",
        "                    alignment_text = (\n",
        "                        \"The description's brevity is consistent with the overall poor quality of the submission, \"\n",
        "                        \"indicating that minimal documentation effort was put into this project.\"\n",
        "                    )\n",
        "                elif desc_cat < overall_category:\n",
        "                    alignment_text = (\n",
        "                        \"While the overall submission is evaluated as higher quality, the description remains relatively brief, \"\n",
        "                        \"suggesting that additional elaboration could have further strengthened its documentation and better supported the project's overall presentation.\"\n",
        "                    )\n",
        "\n",
        "        # BOM part\n",
        "        bom_cat = self.evaluate_label_category(submission_row, same_contest_df, \"num_things\")\n",
        "        bom_syn = self._choose_random_synonym(bom_cat)\n",
        "\n",
        "        if bom_cat == self.CATEGORY_EXCELLENT:\n",
        "            comp_doc = f\"The submission features {bom_syn} documentation of the components, providing a detailed and well-organized list of materials used. \"\n",
        "        elif bom_cat == self.CATEGORY_AVERAGE:\n",
        "            comp_doc = f\"The amount of components used in the project is {bom_syn}, aligning with the average level seen in similar submissions. \"\n",
        "        else:\n",
        "            comp_doc = (\n",
        "                f\"The amount of components is {bom_syn}, indicating either a very basic implementation or potentially missing material details. \"\n",
        "                \"This could hinder the ability to replicate or understand the project’s full scope. \"\n",
        "            )\n",
        "\n",
        "        description_paragraph += f\" {alignment_text} {comp_doc}\"\n",
        "\n",
        "\n",
        "        # --- Documentation Paragraph ---\n",
        "\n",
        "        # Visual Documentation part\n",
        "        visual_labels = [\"num_image\", \"num_video\"]\n",
        "        percentiles = [self.calculate_percentile(submission_row, same_contest_df, label) for label in visual_labels]\n",
        "        avg_percentile = sum(percentiles) / len(percentiles)\n",
        "\n",
        "        if avg_percentile >= self.top_percentile:\n",
        "            visual_category = self.CATEGORY_EXCELLENT\n",
        "        elif avg_percentile >= self.average_percentile:\n",
        "            visual_category = self.CATEGORY_AVERAGE\n",
        "        else:\n",
        "            visual_category = self.CATEGORY_POOR\n",
        "\n",
        "        visual_syn = self._choose_random_synonym(visual_category)\n",
        "        images_present = submission_row[\"num_image\"] > 0\n",
        "        videos_present = submission_row[\"num_video\"] > 0\n",
        "        gifs_present = submission_row[\"num_gif\"] > 0\n",
        "\n",
        "        present_formats = [fmt for fmt, present in zip([\"images\", \"videos\"], [images_present, videos_present]) if present]\n",
        "        format_text = \" and \".join(present_formats) if present_formats else \"visual content\"\n",
        "\n",
        "        # Main visual documentation assessment\n",
        "        if visual_category == self.CATEGORY_EXCELLENT:\n",
        "            visual_doc = (\n",
        "                f\"This entry provides {visual_syn} visual documentation, utilizing many {format_text}. \"\n",
        "                \"The provided media offer a clear representation of the project, making it easier to understand and replicate. \"\n",
        "                \"Compared to other submissions, it includes a notably high amount of visual material.\"\n",
        "            )\n",
        "        elif visual_category == self.CATEGORY_AVERAGE:\n",
        "            visual_doc = (\n",
        "                f\"This entry provides {visual_syn} visual documentation, incorporating a moderate number of {format_text}. \"\n",
        "                \"While the visuals contribute to understanding the project, they are not as extensive as in the highest-ranked submissions. \"\n",
        "                \"Even though the available visuals aid comprehension, additional supporting media could have further enhanced clarity and engagement.\"\n",
        "            )\n",
        "        else:\n",
        "            visual_doc = (\n",
        "                f\"This entry provides {visual_syn} visual documentation, with only limited {format_text} available. \"\n",
        "                \"Compared to other entries, the visual content is sparse, making it more difficult to grasp the project fully. \"\n",
        "                \"A stronger emphasis on visual documentation would have significantly improved the submission’s presentation.\"\n",
        "            )\n",
        "\n",
        "        # Additional checks for missing content\n",
        "        if gifs_present:\n",
        "            visual_doc += \" Additionally, animated GIFs are included, helping to illustrate certain aspects dynamically. \"\n",
        "\n",
        "        missing_resources = [res for res, present in zip([\"images\", \"videos\"], [images_present, videos_present]) if not present]\n",
        "\n",
        "        if missing_resources:\n",
        "            conjunction = \"However\" if visual_category in (self.CATEGORY_EXCELLENT, self.CATEGORY_AVERAGE) else \"Also\"\n",
        "            missing_text = f\" {conjunction}, it lacks key resources such as {', '.join(missing_resources)}. \"\n",
        "\n",
        "            if \"videos\" in missing_resources:\n",
        "                missing_text += (\n",
        "                    \"Without video documentation, understanding how the project operates in real-time is challenging. Including a demonstration video could have showcased key interactions or features more effectively.\"\n",
        "                )\n",
        "\n",
        "            if \"images\" in missing_resources:\n",
        "                missing_text += (\n",
        "                    \" Including images would have helped document the project's components and overall design more effectively, \"\n",
        "                    \"ensuring that viewers can quickly understand its structure and purpose.\"\n",
        "                )\n",
        "\n",
        "            visual_doc += missing_text\n",
        "\n",
        "\n",
        "        # Code\n",
        "        code_provided = (submission_row[\"code\"] == 1)\n",
        "        link_provided = (submission_row[\"link\"] == 1)\n",
        "\n",
        "        if code_provided and link_provided:\n",
        "            code_text = \"It fully shares its code and provides a repository link to encourage collaboration.\"\n",
        "        elif code_provided and not link_provided:\n",
        "            code_text = \"The submission includes some code but lacks a central repository link.\"\n",
        "        else:\n",
        "            code_text = \"Unfortunately, no code is provided, which could leave technical aspects undisclosed.\"\n",
        "\n",
        "        code_doc = f\"{code_text} These factors influence how easily others can reproduce or build upon the work.\"\n",
        "\n",
        "        documentation_paragraph = visual_doc + code_doc\n",
        "\n",
        "        # # --- Paragraph 3: Overall Recommendation ---\n",
        "        if overall_category == self.CATEGORY_EXCELLENT:\n",
        "            disc_classification_str = \"Excellent\"\n",
        "\n",
        "        elif overall_category == self.CATEGORY_AVERAGE:\n",
        "            disc_classification_str = \"Average\"\n",
        "\n",
        "        else:  # Poor Overall Category\n",
        "            disc_classification_str = \"Poor\"\n",
        "\n",
        "        # Final text assembly using list comprehension\n",
        "        paragraphs = [\n",
        "            f\"**Description and bills of materials:**\\n{description_paragraph}\",\n",
        "            f\"**Visuals, code and other documentation:**\\n{documentation_paragraph}\",\n",
        "            f\"**Overall Recommendation:**\\n*{disc_classification_str}*\"\n",
        "        ]\n",
        "\n",
        "        final_text = \"\\n\\n\".join([p for p in paragraphs if p])\n",
        "\n",
        "\n",
        "        return final_text, disc_classification_str\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fD9mUb4aoc_R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Summarizer\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Summarizer\n",
        "# --------------------------------------------------\n",
        "class Summarizer:\n",
        "    \"\"\"\n",
        "    Class Name: Summarizer\n",
        "\n",
        "    Purpose:\n",
        "        - Summarize a given contest description using the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_loader: ModelLoader, data_cleaner: DataCleaner, df=None,\n",
        "                 max_allowed_tokens=100000, summary_size=400):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_loader (ModelLoader): The loaded model/tokenizer wrapper.\n",
        "            data_cleaner (DataCleaner): Utility class for cleaning text & counting tokens.\n",
        "            df (pd.DataFrame, optional): DataFrame with contest info.\n",
        "            max_allowed_tokens (int): Max tokens for prompt+answer.\n",
        "            summary_size (int): Approx. size for the answer portion.\n",
        "        \"\"\"\n",
        "        self.model_loader = model_loader\n",
        "        self.data_cleaner = data_cleaner\n",
        "        self.df = df\n",
        "        self.max_allowed_tokens = max_allowed_tokens\n",
        "        self.summary_size = summary_size\n",
        "\n",
        "    def summarize_contest_description(self, contest_name):\n",
        "        \"\"\"\n",
        "        Summarize the contest description for the given contest_name.\n",
        "\n",
        "        Returns:\n",
        "            str: A cleaned summary of the contest description.\n",
        "        \"\"\"\n",
        "        if self.df is None:\n",
        "            raise ValueError(\"A DataFrame must be provided to Summarizer to look up submissions.\")\n",
        "\n",
        "        # Grab the relevant row\n",
        "        try:\n",
        "            selected_entry = self.df[self.df['contest_name'] == contest_name].iloc[0]\n",
        "        except IndexError:\n",
        "            raise ValueError(f\"Contest name {contest_name} not found in the dataset.\")\n",
        "\n",
        "        # Clean text\n",
        "        contest_description = self.data_cleaner.clean_text(selected_entry['overview'])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "                  You are an expert at summarizing information for clarity and relevance. Below is a description of a contest.\n",
        "                  Your task is to summarize it by focusing on:\n",
        "                  - What is the contest about?\n",
        "                  - Main goals and challenges described?\n",
        "                  - Topics or themes participants address?\n",
        "                  - Overall purpose of the contest?\n",
        "\n",
        "                  Write the summary as a single, continuous paragraph and leave out irrelevant details (prizes, hardware giveaways, registration, etc.).\n",
        "\n",
        "                  Contest Description:\n",
        "                  {contest_description}\n",
        "\n",
        "                  End the Summary with '<|eot_id|>'.\n",
        "                  \"\"\"\n",
        "\n",
        "        prompt_tokens = self.data_cleaner.calculate_token_size(prompt, self.model_loader.tokenizer)\n",
        "        total_max_length = prompt_tokens + self.summary_size\n",
        "        if total_max_length > self.max_allowed_tokens:\n",
        "            raise ValueError(\"Contest description + answer size exceed max token limit.\")\n",
        "\n",
        "        raw_text = self.model_loader.generate_text(\n",
        "            prompt,\n",
        "            max_length=total_max_length,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        cleaned_response = self.data_cleaner.strip_prompt_and_eos_token(raw_text, prompt, self.model_loader.tokenizer)\n",
        "        torch.cuda.empty_cache()\n",
        "        return cleaned_response"
      ],
      "metadata": {
        "id": "u2hxn6QwEeM9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SubmissionEvaluator\n",
        "\n",
        "# --------------------------------------------------\n",
        "# SubmissionEvaluator\n",
        "# --------------------------------------------------\n",
        "class SubmissionEvaluator:\n",
        "    \"\"\"\n",
        "    Class Name: SubmissionEvaluator\n",
        "\n",
        "    Purpose:\n",
        "        - Evaluate a single submission for classification (Excellent, Average, Poor)\n",
        "          using a language model with few-shot examples.\n",
        "        - If no contest_description is provided, it uses Summarizer to generate one.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_loader: ModelLoader, data_cleaner: DataCleaner, df, fewshot_df, fewshot_urls,\n",
        "                 summarizer: Summarizer, output_length=800):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_loader (ModelLoader): Contains the loaded model and tokenizer.\n",
        "            data_cleaner (DataCleaner): Used for cleaning text.\n",
        "            df (pd.DataFrame): Main submissions DataFrame.\n",
        "            fewshot_df (pd.DataFrame): DataFrame containing few-shot examples.\n",
        "            fewshot_urls (list): List of URLs for few-shot examples.\n",
        "            summarizer (Summarizer): Used to generate a contest summary if none is provided.\n",
        "            output_length (int): Maximum length of the output text.\n",
        "        \"\"\"\n",
        "        self.model_loader = model_loader\n",
        "        self.data_cleaner = data_cleaner\n",
        "        self.df = df\n",
        "        self.fewshot_df = fewshot_df\n",
        "        self.fewshot_urls = fewshot_urls\n",
        "        self.summarizer = summarizer\n",
        "        self.output_length = output_length\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_submission(self, submission_url, contest_description=None):\n",
        "        \"\"\"\n",
        "        Evaluate a single submission. Builds a prompt with few-shot examples.\n",
        "        If contest_description=None, we'll use Summarizer to generate one from the submission's contest_name.\n",
        "\n",
        "        Returns:\n",
        "            str: The model-generated text (cleaned).\n",
        "            str: The prompt used.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            selected_entry = self.df[self.df['submission_url'] == submission_url].iloc[0]\n",
        "        except IndexError:\n",
        "            raise ValueError(f\"Submission URL {submission_url} not found in the dataset.\")\n",
        "\n",
        "        # If no contest description, we summarize from the submission's contest_name\n",
        "        if contest_description is None:\n",
        "            contest_name = selected_entry['contest_name']\n",
        "            contest_description = self.summarizer.summarize_contest_description(contest_name)\n",
        "            print(\"Contest Summary:\\n\" + contest_description + \"\\n\")\n",
        "\n",
        "        # Clean text\n",
        "        submission_story = self.data_cleaner.clean_text(selected_entry['story'])\n",
        "\n",
        "        # Prepare few-shot examples\n",
        "        fewshot_submissions_filtered = self.fewshot_df[self.fewshot_df['submission_url'].isin(self.fewshot_urls)]\n",
        "        fewshot_submissions_filtered = fewshot_submissions_filtered.set_index('submission_url') \\\n",
        "                                                                   .reindex(self.fewshot_urls) \\\n",
        "                                                                   .reset_index()\n",
        "        fs = []\n",
        "        for i in range(len(fewshot_submissions_filtered)):\n",
        "            row = fewshot_submissions_filtered.iloc[i]\n",
        "            fs.append({\n",
        "                    \"class\": row['class'],\n",
        "                    \"overview\": row['overview'],\n",
        "                    \"story\": row['story'],\n",
        "                    \"output\": row['expected_response']\n",
        "                })\n",
        "\n",
        "\n",
        "        fewshot_str = f\"\"\"\n",
        "                  --- BEGIN FEW-SHOT EXAMPLES ---\n",
        "                  Below are evaluation examples that illustrate how submissions from other contests are evaluated according to the provided instructions and criteria.\n",
        "                  **Example 1: {fs[0]['class']} Submission**\n",
        "                  Contest Description:\n",
        "                  \"{fs[0]['overview']}\"\n",
        "                  Submission Story:\n",
        "                  \"{fs[0]['story']}\"\n",
        "                  Expected LLM Output:\n",
        "                  \"{fs[0]['output']}\"\n",
        "\n",
        "                  **Example 2: {fs[1]['class']} Submission**\n",
        "                  Contest Description:\n",
        "                  \"{fs[1]['overview']}\"\n",
        "                  Submission Story:\n",
        "                  \"{fs[1]['story']}\"\n",
        "                  Expected LLM Output:\n",
        "                  \"{fs[1]['output']}\"\n",
        "\n",
        "                  **Example 3: {fs[2]['class']} Submission**\n",
        "                  Contest Description:\n",
        "                  \"{fs[2]['overview']}\"\n",
        "                  Submission Story:\n",
        "                  \"{fs[2]['story']}\"\n",
        "                  Expected LLM Output:\n",
        "                  \"{fs[2]['output']}\"\n",
        "                  --- END FEW-SHOT EXAMPLES ---\n",
        "                  \"\"\"\n",
        "\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "                  You are an expert evaluator for technical contests. Your task is to assess a submission based on the following:\n",
        "\n",
        "                  --- BEGIN INSTRUCTIONS ---\n",
        "                  1. Provide a structured evaluation consisting of two concise paragraphs, each addressing one of the following criteria in a few sentences:\n",
        "                    - *Novelty of the Solution*: Evaluate how novel the solution is. Search for similar, existing solutions, and evaluate how different and unique this solution is compared to those existing solutions. Identify any concept, feature, technology or approach that might be novel.\n",
        "                    - *Usefulness of the Solution*: Evaluate how useful the solution is. Consider factors such as practicality, usability, and relevance. Identify potential challenges that might hinder its real-world value.\n",
        "\n",
        "                  2. Choose one of the following overall recommendations. Be critical in your evaluations. If a solution does not clearly surpass existing alternatives, it should not be rated as 'Excellent.' Carefully consider any limitation before rating a solution as even 'Average.' About 80% of the solutions should be rated as 'Average' or 'Poor.'\n",
        "                    - Excellent: The solution demonstrates both substantial novelty and usefulness, far exceeding typical expectations.\n",
        "                    - ⁠Average: The solution demonstrates a reasonable degree of novelty and usefulness, meeting typical expectations without exceeding them.\n",
        "                    - ⁠Poor: The solution is only moderately novel or useful, and is thus unlikely to meet typical expectations.\n",
        "                  --- END INSTRUCTIONS ---\n",
        "\n",
        "                  {fewshot_str}\n",
        "\n",
        "                  --- BEGIN SUBMISSION TO EVALUATE ---\n",
        "                  Contest Description:\n",
        "                  {contest_description}\n",
        "\n",
        "                  Submission Story:\n",
        "                  {submission_story}\n",
        "                  --- END SUBMISSION TO EVALUATE ---\n",
        "\n",
        "                  End your response with '<|eot_id|>'.\n",
        "                  \"\"\"\n",
        "\n",
        "        prompt_tokens = self.data_cleaner.calculate_token_size(prompt, self.model_loader.tokenizer)\n",
        "        total_max_length = prompt_tokens + self.output_length\n",
        "        stop_token_ids = torch.tensor([128000, 27, 91, 9684, 91, 29])\n",
        "\n",
        "        response = self.model_loader.generate_text(\n",
        "            prompt,\n",
        "            max_length=total_max_length,\n",
        "            temperature=0.7,\n",
        "            stop_token_ids=stop_token_ids\n",
        "        )\n",
        "        cleaned_response = DataCleaner.strip_prompt_and_eos_token(response, prompt, self.model_loader.tokenizer)\n",
        "        torch.cuda.empty_cache()\n",
        "        return cleaned_response, prompt\n"
      ],
      "metadata": {
        "id": "OKcrO-CHU_3a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ContestEvaluator\n",
        "\n",
        "# --------------------------------------------------\n",
        "# ContestEvaluator\n",
        "# --------------------------------------------------\n",
        "class ContestEvaluator:\n",
        "    \"\"\"\n",
        "    Orchestrates evaluation of all submissions in a given contest.\n",
        "    Saves results with extra columns: \"contest_description\", \"discriminative\",\n",
        "    \"disc_classification\", \"generative\", \"gen_classification\", ...\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, submission_evaluator, discriminative_evaluator, summarizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            submission_evaluator: Evaluates single submissions (generative).\n",
        "            discriminative_evaluator: Numeric-based evaluation text + classification.\n",
        "            summarizer: Summarizes the contest if no description is given.\n",
        "        \"\"\"\n",
        "        self.submission_evaluator = submission_evaluator\n",
        "        self.discriminative_evaluator = discriminative_evaluator\n",
        "        self.summarizer = summarizer\n",
        "\n",
        "    def parse_llm_response(self, llm_response):\n",
        "        \"\"\"\n",
        "        Extract classification label (Excellent, Average, or Poor) enclosed in **...**,\n",
        "        returning the last occurrence in the text.\n",
        "        Example matches: **Excellent**, **Average**, **Poor** (case-insensitive).\n",
        "        \"\"\"\n",
        "        # Find all occurrences of **Excellent**, **Average**, or **Poor** (case-insensitive)\n",
        "        matches = re.findall(r\"(Excellent|Average|Poor)\", llm_response, re.IGNORECASE)\n",
        "        if not matches:\n",
        "            return None\n",
        "        # Take the last match and capitalize it (e.g., \"Excellent\", \"Average\", or \"Poor\")\n",
        "        return matches[-1].capitalize()\n",
        "\n",
        "    def evaluate_full_contest(self, contest_name, df, output_csv_path=None, contest_description=None):\n",
        "        \"\"\"\n",
        "        Evaluate all submissions in a contest. Adds:\n",
        "            \"disc_classification\" from DiscriminativeEvaluator\n",
        "            \"gen_classification\" from parse_llm_response.\n",
        "        \"\"\"\n",
        "        if contest_description is None:\n",
        "            contest_description = self.summarizer.summarize_contest_description(contest_name)\n",
        "\n",
        "        # Filter for the target contest\n",
        "        contest_df = df[df['contest_name'] == contest_name].copy()\n",
        "        # Sort by 'quality_score' descending\n",
        "        contest_df = contest_df.sort_values(by='qs_raw_avg', ascending=False)\n",
        "        # # For demonstration, selecting bottom 50% only (custom logic)\n",
        "        # contest_df = contest_df.iloc[len(contest_df) // 2:]\n",
        "\n",
        "        results_df = pd.DataFrame(columns=[\n",
        "            \"submission_url\", \"submission_name\", \"contest_name\",\n",
        "            \"num_of_submissions\", \"rank\", \"quality_score\",\n",
        "            \"contest_description\",\n",
        "            \"prompt\",\n",
        "            \"discriminative\",\n",
        "            \"disc_classification\",\n",
        "            \"generative\",\n",
        "            \"gen_classification\",\n",
        "            \"winner_categories\"\n",
        "        ])\n",
        "\n",
        "        for idx, row in tqdm(contest_df.iterrows(), total=len(contest_df), desc=\"Evaluating submissions\"):\n",
        "            submission_url = row['submission_url']\n",
        "            submission_name = row['submission_name']\n",
        "            num_submissions = row['num_of_submissions']\n",
        "            rank = row['qs_rank']\n",
        "            quality_score = row['qs_raw_avg']\n",
        "            winner_categories = row['winner_categories']\n",
        "\n",
        "            # (1) Discriminative\n",
        "            disc_text, disc_classification = self.discriminative_evaluator.discriminative_evaluation(submission_url)\n",
        "\n",
        "            # (2) Generative\n",
        "            try:\n",
        "                generative_resp, prompt = self.submission_evaluator.evaluate_submission(\n",
        "                    submission_url,\n",
        "                    contest_description=contest_description\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {submission_url}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # (3) Extract generative classification\n",
        "            gen_classification = self.parse_llm_response(generative_resp)\n",
        "\n",
        "            new_row = {\n",
        "                \"submission_url\": submission_url,\n",
        "                \"submission_name\": submission_name,\n",
        "                \"contest_name\": contest_name,\n",
        "                \"num_of_submissions\": num_submissions,\n",
        "                \"rank\": rank,\n",
        "                \"quality_score\": quality_score,\n",
        "                \"contest_description\": contest_description,\n",
        "                \"prompt\": prompt,\n",
        "                \"discriminative\": disc_text,\n",
        "                \"disc_classification\": disc_classification,\n",
        "                \"generative\": generative_resp,\n",
        "                \"gen_classification\": gen_classification,\n",
        "                \"winner_categories\": winner_categories\n",
        "            }\n",
        "\n",
        "            results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "\n",
        "            if output_csv_path:\n",
        "                results_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "        return results_df"
      ],
      "metadata": {
        "id": "w1zumW9xNmOZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ContestSummaryPlotter\n",
        "\n",
        "# --------------------------------------------------\n",
        "# ContestSummaryPlotter\n",
        "# --------------------------------------------------\n",
        "\n",
        "class ContestSummaryPlotter:\n",
        "    \"\"\"\n",
        "    Generates textual summaries and plots based on contest evaluation results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_path, contest_name, model_name):\n",
        "        \"\"\"\n",
        "        Initializes the class with the contest data.\n",
        "\n",
        "        Args:\n",
        "            csv_path (str): Path to the CSV file containing contest evaluation results.\n",
        "            contest_name (str): Name of the contest.\n",
        "            model_name (str): Name of the model used for evaluation.\n",
        "        \"\"\"\n",
        "        self.csv_path = csv_path\n",
        "        self.contest_name = contest_name\n",
        "        self.model_name = model_name\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Replace NaN values in classification columns\n",
        "        self.df['gen_classification'] = self.df['gen_classification'].fillna(\"Poor\")\n",
        "        self.df['disc_classification'] = self.df['disc_classification'].fillna(\"Poor\")\n",
        "\n",
        "        # Ensure winner_categories is treated as an integer\n",
        "        self.df['winner_categories'] = self.df['winner_categories'].astype(int)\n",
        "\n",
        "        # Mapping of prize categories\n",
        "        self.category_names = {\n",
        "            2: \"Top Prize\",\n",
        "            1: \"Mediocre Prize\",\n",
        "            0: \"No Prize\"\n",
        "        }\n",
        "\n",
        "    def print_classification_totals(self):\n",
        "        \"\"\"Prints the overall classification totals for both generative and discriminative evaluations.\"\"\"\n",
        "        print(\"=== Generative Classification Totals ===\")\n",
        "        print(f\"Excellent: {sum(self.df['gen_classification'] == 'Excellent')}\")\n",
        "        print(f\"Average:   {sum(self.df['gen_classification'] == 'Average')}\")\n",
        "        print(f\"Poor:      {sum(self.df['gen_classification'] == 'Poor')}\\n\")\n",
        "\n",
        "        print(\"=== Discriminative Classification Totals ===\")\n",
        "        print(f\"Excellent: {sum(self.df['disc_classification'] == 'Excellent')}\")\n",
        "        print(f\"Average:   {sum(self.df['disc_classification'] == 'Average')}\")\n",
        "        print(f\"Poor:      {sum(self.df['disc_classification'] == 'Poor')}\\n\")\n",
        "\n",
        "    def plot_classification_distribution(self, classification_col, title_suffix):\n",
        "        \"\"\"\n",
        "        Creates a bar plot showing classification distribution for different prize categories,\n",
        "        including total counts for each classification in the legend.\n",
        "\n",
        "        Args:\n",
        "            classification_col (str): Column name to plot ('gen_classification' or 'disc_classification').\n",
        "            title_suffix (str): Title suffix indicating whether it's for generative or discriminative classification.\n",
        "        \"\"\"\n",
        "        # Compute counts for each prize category and classification\n",
        "        counts = self.df.groupby([\"winner_categories\", classification_col]).size().unstack(fill_value=0)\n",
        "        counts.index = counts.index.map(self.category_names)\n",
        "\n",
        "        # Ensure all classification categories are present\n",
        "        classification_order = [\"Excellent\", \"Average\", \"Poor\"]\n",
        "        for cls in classification_order:\n",
        "            if cls not in counts.columns:\n",
        "                counts[cls] = 0\n",
        "        counts = counts[classification_order]\n",
        "\n",
        "        # Compute total numbers for each classification\n",
        "        total_counts = self.df[classification_col].value_counts().reindex(classification_order, fill_value=0)\n",
        "\n",
        "        # Create the plot\n",
        "        ax = counts.plot(kind='bar', stacked=False, figsize=(8, 6))\n",
        "\n",
        "        # Modify legend to include totals\n",
        "        legend_labels = [f\"{cls} (Total: {total_counts[cls]})\" for cls in classification_order]\n",
        "        ax.legend(legend_labels, title=\"Classification\")\n",
        "\n",
        "        # # Add a text box with totals\n",
        "        # total_text = \"\\n\".join([f\"{cls}: {total_counts[cls]}\" for cls in classification_order])\n",
        "        # props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
        "        # ax.text(0.95, 0.95, total_text, transform=ax.transAxes, fontsize=10,\n",
        "        #         verticalalignment='top', horizontalalignment='right', bbox=props)\n",
        "\n",
        "        # Set plot labels and title\n",
        "        ax.set_xlabel(\"Prize Category\")\n",
        "        ax.set_ylabel(\"Number of Classifications\")\n",
        "        ax.set_ylabel(\"Number of Classifications\")\n",
        "        if \"Discriminative\" in title_suffix:\n",
        "            ax.set_title(f\"{title_suffix} - {self.contest_name}\")\n",
        "        else:\n",
        "            ax.set_title(f\"{title_suffix} - {self.contest_name} with {self.model_name}\")\n",
        "\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_combined_classification(self):\n",
        "        \"\"\"Creates a combined bar chart comparing generative and discriminative classifications.\"\"\"\n",
        "        gen_counts = self.df.groupby([\"winner_categories\", \"gen_classification\"]).size().unstack(fill_value=0)\n",
        "        disc_counts = self.df.groupby([\"winner_categories\", \"disc_classification\"]).size().unstack(fill_value=0)\n",
        "\n",
        "        gen_counts.index = gen_counts.index.map(self.category_names)\n",
        "        disc_counts.index = disc_counts.index.map(self.category_names)\n",
        "\n",
        "        classification_labels = [\"Excellent\", \"Average\", \"Poor\"]\n",
        "        for cls_label in classification_labels:\n",
        "            if cls_label not in gen_counts.columns:\n",
        "                gen_counts[cls_label] = 0\n",
        "            if cls_label not in disc_counts.columns:\n",
        "                disc_counts[cls_label] = 0\n",
        "\n",
        "        gen_counts = gen_counts[classification_labels]\n",
        "        disc_counts = disc_counts[classification_labels]\n",
        "\n",
        "        gen_counts.columns = pd.MultiIndex.from_product([['Generative'], gen_counts.columns])\n",
        "        disc_counts.columns = pd.MultiIndex.from_product([['Discriminative'], disc_counts.columns])\n",
        "\n",
        "        combined = pd.concat([gen_counts, disc_counts], axis=1)\n",
        "\n",
        "        ax = combined.plot(kind='bar', stacked=False, figsize=(10, 6))\n",
        "        ax.set_xlabel(\"Prize Category\")\n",
        "        ax.set_ylabel(\"Number of Classifications\")\n",
        "        ax.set_title(f\"Combined Generative & Discriminative Classification - {self.contest_name} with {self.model_name}\")\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_summary_and_plot(self):\n",
        "        \"\"\"Executes all summary printing and plotting functions.\"\"\"\n",
        "        self.print_classification_totals()\n",
        "        self.plot_classification_distribution(\"gen_classification\", \"Generative Classification\")\n",
        "        self.plot_classification_distribution(\"disc_classification\", \"Discriminative Classification\")\n",
        "        self.plot_combined_classification()\n"
      ],
      "metadata": {
        "id": "u0zdl0pxhw9i",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive_mgr = DriveManager()\n",
        "drive_mgr.mount_drive(force_remount=True)\n",
        "\n",
        "# Load  CSVs\n",
        "input_path = '/content/drive/MyDrive/Master_Thesis/LLM_Input/'\n",
        "dataset_path = os.path.join(input_path, 'all_submissions_averaged_qs.csv')\n",
        "\n",
        "df = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "SOn-yT9nf9K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Creation of FewShot CSV\n",
        "\n",
        "# List of submission URLs to extract\n",
        "submission_urls = [\n",
        "    # \"https://www.hackster.io/dxcfl/fall-detection-alert-131971\",\n",
        "    # \"https://www.hackster.io/davide-cogliati/careful-please-i-am-here-i-am-moving-respect-658e23\",\n",
        "    \"https://www.hackster.io/PSoC_Rocks/gaming-with-disabilities-face-tracking-game-controller-8128d8\",\n",
        "    # \"https://www.hackster.io/oneohm/ohmni-stick-feec50\",\n",
        "    # \"https://www.hackster.io/alistair/tell-me-my-time-c386b3\",\n",
        "    # \"https://www.hackster.io/rhammell/underwater-sound-beacons-for-visually-impaired-swimmers-b1aa85\",\n",
        "    # \"https://www.hackster.io/bltrobotics/alexa-the-english-major-thesaurus-skill-b14c7e\",\n",
        "    \"https://www.hackster.io/team-decode/large-language-model-on-tamil-literature-a23d0d\",\n",
        "    \"https://www.hackster.io/jayakarthigeyan/smart-desk-clock-to-save-power-using-iot-at-home-office-cd1695\"\n",
        "]\n",
        "\n",
        "# Filter the DataFrame for the given submission URLs\n",
        "filtered_df = df[df[\"submission_url\"].isin(submission_urls)]\n",
        "\n",
        "# Select the required labels and add the new 'expected_response' column\n",
        "filtered_df = filtered_df[\n",
        "    [\n",
        "        \"overview\",\n",
        "        \"submission_url\",\n",
        "        \"submission_word_count\",\n",
        "        \"num_image\",\n",
        "        \"num_gif\",\n",
        "        \"num_video\",\n",
        "        \"video_duration\",\n",
        "        \"num_things\",\n",
        "        \"cad\",\n",
        "        \"schematic\",\n",
        "        \"code\",\n",
        "        \"code_lines\",\n",
        "        \"link\",\n",
        "        \"story\",\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Add the new column 'expected_response' with empty values\n",
        "filtered_df[\"expected_response\"] = \"\"\n",
        "filtered_df[\"class\"] = \"\"\n",
        "\n",
        "# Update entries for specific URLs\n",
        "updates = {\n",
        "#       \"https://www.hackster.io/dxcfl/fall-detection-alert-131971\": {\n",
        "#         \"class\": \"\"\"Poor\"\"\",\n",
        "#         \"overview\": \"\"\"The contest is an innovation challenge focused on developing assistive technologies to improve the lives of individuals with mobility impairments. Participants are tasked with identifying gaps in existing solutions and designing novel and feasible solutions. Ultimately, the contest aims to advance assistive technologies and drive innovation toward a more inclusive world for individuals with mobility impairments.\"\"\",\n",
        "#         \"expected_response\": \"\"\"\n",
        "# **Novelty of the Solution:**\n",
        "# This solution lacks novelty, as fall detection systems using accelerometers and gyroscopes have been extensively researched and commercially implemented for years. Many existing devices, such as smartwatches and medical alert systems, already employ similar heuristics to detect falls and send alerts. The submission does not introduce any significantly new concepts, features, or approaches that differentiate it from widely available solutions.\n",
        "\n",
        "# **Usefulness of the Solution:**\n",
        "# While fall detection is important, this implementation does not offer substantial improvements over existing technologies. The reliance on specific hardware components, including the Sony Spresense LTE board, limits accessibility and raises concerns about practicality compared to widely adopted alternatives like wearable smart devices with built-in emergency alert functions. Additionally, the system lacks evidence of user testing or validation, which is crucial for real-world adoption.\n",
        "\n",
        "# **Overall Recommendation:**\n",
        "# *Poor*\n",
        "# \"\"\"\n",
        "#     },\n",
        "#       \"https://www.hackster.io/davide-cogliati/careful-please-i-am-here-i-am-moving-respect-658e23\": {\n",
        "#         \"class\": \"\"\"Poor\"\"\",\n",
        "#         \"overview\": \"\"\"The contest is an innovation challenge focused on developing assistive technologies to improve the lives of individuals with mobility impairments. Participants are tasked with identifying gaps in existing solutions and designing novel and feasible solutions. Ultimately, the contest aims to advance assistive technologies and drive innovation toward a more inclusive world for individuals with mobility impairments.\"\"\",\n",
        "#         \"expected_response\": \"\"\"\n",
        "# **Novelty of the Solution:**\n",
        "# This solution lacks novelty, as visibility aids for wheelchairs, such as LED lights, reflective strips, and wearable flashing devices, already exist. The use of a Microbit for displaying messages is not significantly different from other programmable LED solutions available. While the idea of using a Fresnel lens to enlarge the display is somewhat unique, it does not introduce a groundbreaking improvement in assistive technology. The concept remains a simple adaptation of existing tools rather than an innovative advancement.\n",
        "\n",
        "# **Usefulness of the Solution:**\n",
        "# The practicality of this solution is limited. While increasing visibility is important for wheelchair users, the proposed implementation may not be effective in real-world conditions. A small LED matrix, even with a Fresnel lens, may not be bright enough to be noticed in daylight or busy environments. Additionally, requiring users to program their own messages could be a barrier for those with limited technical skills. More robust solutions, such as larger, high-visibility lights or integrated alert systems, would offer better usability and reliability for mobility-impaired individuals.\n",
        "\n",
        "# **Overall Recommendation:**\n",
        "# *Poor*\n",
        "# \"\"\"\n",
        "#     },\n",
        "      \"https://www.hackster.io/PSoC_Rocks/gaming-with-disabilities-face-tracking-game-controller-8128d8\": {\n",
        "        \"class\": \"\"\"Average\"\"\",\n",
        "        \"overview\": \"\"\"The contest is an innovation challenge focused on developing assistive technologies to improve the lives of individuals with mobility impairments. Participants are tasked with identifying gaps in existing solutions and designing novel and feasible solutions. Ultimately, the contest aims to advance assistive technologies and drive innovation toward a more inclusive world for individuals with mobility impairments.\"\"\",\n",
        "        \"expected_response\": \"\"\"\n",
        "**Novelty of the Solution:**\n",
        "The proposed hands-free gaming controller offers some level of innovation by integrating face tracking and optical sensors for user input. While gesture and facial tracking for assistive gaming already exist, this project attempts to simplify implementation with accessible hardware and a microcontroller-based approach. However, similar concepts have been explored in adaptive gaming technologies, including commercial eye-tracking and motion-based controllers. The addition of laser-interrupt buttons is an interesting touch, but it does not significantly push the boundaries of existing solutions.\n",
        "\n",
        "**Usefulness of the Solution:**\n",
        "The solution has practical value for users with mobility impairments, particularly those who struggle with traditional controllers. The use of face tracking and air-tap buttons makes gaming more accessible. However, the prototype is incomplete, with noticeable latency issues and missing features such as servo motor control. The reliance on specific hardware components may also limit scalability. While the idea is promising, further refinement is needed to enhance usability and responsiveness before it can be considered a fully functional assistive device.\n",
        "\n",
        "**Overall Recommendation:**\n",
        "*Average*\n",
        "\"\"\"\n",
        "    },\n",
        "#         \"https://www.hackster.io/oneohm/ohmni-stick-feec50\": {\n",
        "#         \"class\": \"\"\"Excellent\"\"\",\n",
        "#         \"overview\": \"\"\"The contest is an innovation challenge focused on developing assistive technologies to improve the lives of individuals with mobility impairments. Participants are tasked with identifying gaps in existing solutions and designing novel and feasible solutions. Ultimately, the contest aims to advance assistive technologies and drive innovation toward a more inclusive world for individuals with mobility impairments.\"\"\",\n",
        "#         \"expected_response\": \"\"\"\n",
        "# **Novelty of the Solution:**\n",
        "# The OHMni-Stick presents a highly innovative approach to assistive technology by integrating a low-force joystick with pneumatic pressure-sensitive buttons. While adaptive input devices exist, this solution stands out due to its modular, swappable grip system and sip-and-puff controls, which offer unparalleled customization for users with diverse mobility impairments. The open-source nature of the project fosters collaboration, ensuring continued refinement and accessibility. The combination of ultra-sensitive input, real-time tracking, and customizable ergonomics demonstrates a level of novelty that surpasses typical assistive input devices.\n",
        "\n",
        "# **Usefulness of the Solution:**\n",
        "# This device significantly enhances accessibility by addressing major barriers faced by individuals with limited mobility. Its intuitive and adaptable design ensures ease of use, while the lightweight force requirements make it practical for a broad range of users. The swappable grip system and multiple control options provide flexibility, ensuring that users can personalize their experience to suit their needs. The open-source aspect not only promotes innovation but also reduces costs, making the solution more accessible. The only potential challenge lies in manufacturing scalability, but the project’s open documentation mitigates this by encouraging widespread adoption and improvement.\n",
        "\n",
        "# **Overall Recommendation:**\n",
        "# *Excellent*\n",
        "# \"\"\"\n",
        "    # },\n",
        "#     \"https://www.hackster.io/alistair/tell-me-my-time-c386b3\": {\n",
        "#         \"class\": \"\"\"Poor\"\"\",\n",
        "#         \"overview\": \"\"\"The Inclusive Innovation Challenge Ultimate Award is a contest that seeks to design assistive technologies that address the unique needs of individuals with disabilities. Participants are encouraged to submit solutions for three themes GAMING for people with mobility impairments TRAVELLING for people with mobility impairments and SWIMMING for people with visual impairments. The contest aims to revolutionize the assistive technologies landscape by actively involving individuals with disabilities in every stage of product design and prototyping. The ultimate goal is to drive innovation and shape a future where assistive technologies are truly groundbreaking and inclusive.\"\"\",\n",
        "#         \"expected_response\": \"\"\"\n",
        "# **Novelty of the Solution:**\n",
        "# This solution lacks significant novelty. Gesture-based assistive technologies have been widely explored, particularly for accessibility applications. Existing solutions, such as motion-tracking wearables and voice-activated interfaces, already provide similar functionality. The use of a gesture-based system to announce time is not an innovative breakthrough, as similar technologies exist in smart home assistants and fitness tracking devices. The approach taken does not present a substantially unique feature or advancement in assistive technology.\n",
        "\n",
        "# **Feasibility of the Solution:**\n",
        "# The solution presents notable feasibility challenges. The reliance on a Google Coral Dev Board Mini for pose estimation introduces hardware dependencies that limit scalability and accessibility for a wider user base. Additionally, real-time gesture recognition in dynamic environments such as swimming pools poses technical difficulties, including lighting variations and motion inconsistencies in water. The requirement for external hardware, setup complexity, and the potential for misinterpretation of gestures further hinder practical adoption.\n",
        "\n",
        "# **Overall Recommendation:**\n",
        "# This submission falls short in both novelty and feasibility. The proposed solution does not offer a distinct advantage over existing assistive technologies, and its technical implementation presents significant usability concerns. The limited scope of its functionality, combined with its hardware constraints, makes it an impractical choice for widespread adoption. Overall, this submission is rated as *Poor*.\n",
        "# \"\"\"\n",
        "#     },\n",
        "#     \"https://www.hackster.io/rhammell/underwater-sound-beacons-for-visually-impaired-swimmers-b1aa85\": {\n",
        "#         \"class\": \"\"\"Excellent\"\"\",\n",
        "#         \"overview\": \"\"\"The Inclusive Innovation Challenge Ultimate Award is a contest that seeks to design assistive technologies that address the unique needs of individuals with disabilities. Participants are encouraged to submit solutions for three themes GAMING for people with mobility impairments TRAVELLING for people with mobility impairments and SWIMMING for people with visual impairments. The contest aims to revolutionize the assistive technologies landscape by actively involving individuals with disabilities in every stage of product design and prototyping. The ultimate goal is to drive innovation and shape a future where assistive technologies are truly groundbreaking and inclusive.\"\"\",\n",
        "#         \"expected_response\": \"\"\"\n",
        "# **Novelty of the Solution:**\n",
        "# This solution exhibits a high degree of novelty in the field of assistive swimming technologies. While audio cues have been utilized in other accessibility applications, the integration of a directional underwater beacon system for visually impaired swimmers is a significant innovation. Unlike existing wearable solutions or tactile lane markers, this system provides real-time spatial awareness without requiring physical contact or additional wearable devices. The ability to configure multiple beacons to emit different cues enhances its adaptability, making it a groundbreaking approach to improving accessibility in aquatic environments.\n",
        "\n",
        "# **Feasibility of the Solution:**\n",
        "# The solution is highly feasible due to its practical design and straightforward deployment process. The use of readily available hardware, including the Adafruit Feather RP2040 and a mini transducer, ensures cost-effectiveness and ease of manufacturing. The waterproof, anchored design minimizes interference with standard pool operations, making it a scalable solution for public and private swimming facilities. Additionally, the system's customizable audio cues and potential for synchronized beacon placement demonstrate strong adaptability for different user needs. The only potential challenge could be ensuring consistent audio clarity in varying water conditions, but this is a manageable technical hurdle.\n",
        "\n",
        "# **Overall Recommendation:**\n",
        "# This submission represents an outstanding example of technical innovation in assistive technology. It effectively combines novelty, practicality, and user-centered design, addressing a critical accessibility gap in swimming for visually impaired individuals. The careful integration of hardware and software, along with its ease of deployment, makes it highly viable for real-world application. Overall, this submission is rated as *Excellent*.\n",
        "# \"\"\"\n",
        "#     }\n",
        "    # ,\n",
        "    \"https://www.hackster.io/team-decode/large-language-model-on-tamil-literature-a23d0d\": {\n",
        "        \"class\": \"\"\"Poor\"\"\",\n",
        "        \"overview\": \"\"\"The AMD Pervasive AI Developer Contest is an innovation challenge for developers to create AI projects using AMD hardware and software. The contest has three categories Generative AI Robotics AI and PC AI each with a top prize of 10000 and additional prizes for second and thirdplace winners. Participants can use AMDs ROCm software Kria KR260 Robotics Starter Kit and Ryzen AIpowered PCs to develop innovative applications. The contest aims to foster innovation in academia and industry and participants can engage with AMD technical experts connect with fellow developers and showcase their abilities and innovations to a broader audience. The contest is open to developers worldwide with a special prize for university students and a Women in Technology award for teams with a majority of women. The contest requires participants to create a project document it and submit it by June 30 2024.\"\"\",\n",
        "        \"expected_response\": \"\"\"\n",
        "**Novelty of the Solution:**\n",
        "The submission lacks significant novelty, as fine-tuning large language models (LLMs) on specific languages is a well-explored domain. Many existing projects have already adapted LLMs for regional languages, including Tamil, using open-source models. While language-specific optimization is valuable, the approach presented here does not introduce any novel techniques, methodologies, or architectural improvements beyond standard fine-tuning practices.\n",
        "\n",
        "**Usefulness of the Solution:**\n",
        "The usefulness of the solution is limited due to the lack of a well-defined dataset and a clear implementation plan. Without a proper dataset, the fine-tuning process may not produce meaningful improvements over existing multilingual models, which already include Tamil text to some extent. Additionally, the submission does not clarify how the model's performance was evaluated or how it outperforms alternatives in real-world applications. Without these details, the solution appears incomplete and impractical for widespread adoption.\n",
        "\n",
        "**Overall Recommendation:** *Poor*\n",
        "\"\"\"\n",
        "    },\n",
        "#        \"https://www.hackster.io/bltrobotics/alexa-the-english-major-thesaurus-skill-b14c7e\": {\n",
        "#         \"class\": \"\"\"Average\"\"\",\n",
        "#         \"overview\": \"\"\"The contest is about Alexa Skill Contest which encourages innovation in voice technology using Alexa. The contest aims to connect developers with IoT platforms and everyday life allowing users to interact with devices in a more intuitive way. Participants are invited to build and submit Amazon Alexa Skills that demonstrate creative and innovative uses of voice user interfaces. The contest is judged based on creativity use of VUI best practices project documentation code and great visuals. Bonus points are awarded for using AWS Lambda to build the project. The contest is open to developers worldwide with prizes available for US participants only.\"\"\",\n",
        "#         \"expected_response\": \"\"\"\n",
        "# **Alignment with Contest Theme**\n",
        "# This Alexa Skill directly responds to the contest’s call for creativity in voice technology. It uses Amazon’s Voice User Interface (VUI) and follows standard best practices laid out for Alexa Skills development. While it doesn’t delve deeply into IoT integration or more advanced functionalities, it still adheres to the fundamental goal of enhancing user interaction through voice commands.\n",
        "\n",
        "# **Novelty of the Submission**\n",
        "# Offering a thesaurus function via Alexa is a practical idea, but it’s not particularly groundbreaking. Skills providing dictionary or synonym lookups have existed for a while, so the approach does not introduce a clearly innovative twist. Still, the idea of structuring synonyms by part of speech may offer a modest layer of differentiation, but overall uniqueness is limited.\n",
        "\n",
        "# **Feasibility of the Submission**\n",
        "# From a technical standpoint, the project is straightforward and highly feasible. The required architecture—using AWS Lambda for backend logic and Alexa’s Custom Interaction Model—has well-documented setup guides. This simplicity minimizes cost and infrastructure concerns, making the solution both cost-efficient and easy to maintain.\n",
        "\n",
        "# **Overall Recommendation**\n",
        "# In terms of alignment with the contest theme, the submission is on target by demonstrating a functional Alexa Skill. However, it lacks significant novelty beyond basic word lookup functionality. Its feasibility is solid, but the overall impact does not stand out among potentially more creative or complex entries. As a result, the submission is classified as **Average**.\n",
        "# \"\"\"\n",
        "#     }\n",
        "#     ,\n",
        "    \"https://www.hackster.io/jayakarthigeyan/smart-desk-clock-to-save-power-using-iot-at-home-office-cd1695\": {\n",
        "        \"class\": \"\"\"Excellent\"\"\",\n",
        "        \"overview\": \"\"\"The contest, sponsored by Infineon, is about building industrial applications using Infineon's semiconductor and sensor technology. The main goals and challenges are to create innovative solutions in various categories, including Smart Home, Sports, Fitness, Healthcare, Elderly Care, Smart Cities, and Automotive. Participants are encouraged to address topics such as indoor and outdoor navigation, health and sports devices, drones with precision altitude control, and responsive gaming devices. The overall purpose of the contest is to showcase the potential of Infineon's products and encourage developers to build industrial applications using them. Participants can submit their projects by October 3rd, and the winners will be announced on October 12th.\"\"\",\n",
        "        \"expected_response\": \"\"\"\n",
        "**Novelty of the Solution:**\n",
        "The Smart Desk Clock presents a highly innovative approach by integrating multiple advanced sensors, including Infineon's DPS310 pressure sensor and 24GHz radar, to enable real-time automation and monitoring. While smart home devices exist, this solution uniquely combines motion tracking, IR control, and IoT connectivity into a single compact system. The ability to intelligently adjust room conditions based on occupancy and movement, while also providing multiple operational modes, sets it apart from conventional smart home solutions.\n",
        "\n",
        "**Usefulness of the Solution:**\n",
        "The practicality of this solution is exceptional, addressing real-world challenges such as energy efficiency, security, and convenience. Its ability to automate appliances, provide intrusion alerts, and offer personalized lighting modes enhances its usability across various environments. The inclusion of MQTT-based communication allows seamless integration with other IoT devices, further increasing its adaptability. Potential challenges, such as complex setup and reliance on specific hardware, are minor compared to the overall functionality and benefits offered.\n",
        "\n",
        "**Overall Recommendation:** *Excellent*\n",
        "\"\"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Apply the updates to the filtered DataFrame\n",
        "for url, update in updates.items():\n",
        "    entry_index = filtered_df[filtered_df[\"submission_url\"] == url].index\n",
        "    if not entry_index.empty:\n",
        "        filtered_df.loc[entry_index, \"overview\"] = update[\"overview\"]\n",
        "        filtered_df.loc[entry_index, \"expected_response\"] = update[\"expected_response\"]\n",
        "        filtered_df.loc[entry_index, \"class\"] = update[\"class\"]\n",
        "\n",
        "\n",
        "\n",
        "# Save the filtered DataFrame to a new CSV file\n",
        "fewshot_path = os.path.join(input_path, 'fewshot_submissions.csv')\n",
        "filtered_df.to_csv(fewshot_path, index=False)\n",
        "\n",
        "print(f\"Fewshot examples have been saved to {fewshot_path}\")\n",
        "\n",
        "\n",
        "fewshot_df = pd.read_csv(fewshot_path)"
      ],
      "metadata": {
        "id": "53Zo6ISZuHxL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model/Tokenizer\n",
        "# model_loader = ModelLoader(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "model_loader = ModelLoader(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# model_loader = ModelLoader(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
        "# model_loader = ModelLoader(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
        "# model_loader = ModelLoader(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
        "\n",
        "\n",
        "model_loader.load_model_and_tokenizer()"
      ],
      "metadata": {
        "id": "hO4Hk4vnX6eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the urls of the submissions you want as fewshot examples (they have to be in the fewshot_df)\n",
        "fewshot_urls= [\n",
        "    # \"https://www.hackster.io/dxcfl/fall-detection-alert-131971\",\n",
        "    # \"https://www.hackster.io/davide-cogliati/careful-please-i-am-here-i-am-moving-respect-658e23\",\n",
        "    # \"https://www.hackster.io/PSoC_Rocks/gaming-with-disabilities-face-tracking-game-controller-8128d8\",\n",
        "    # \"https://www.hackster.io/oneohm/ohmni-stick-feec50\",\n",
        "    \"https://www.hackster.io/PSoC_Rocks/gaming-with-disabilities-face-tracking-game-controller-8128d8\",\n",
        "    \"https://www.hackster.io/team-decode/large-language-model-on-tamil-literature-a23d0d\",\n",
        "    \"https://www.hackster.io/jayakarthigeyan/smart-desk-clock-to-save-power-using-iot-at-home-office-cd1695\"\n",
        "]\n",
        "\n",
        "# Choose output length of the LLM\n",
        "output_length=600"
      ],
      "metadata": {
        "id": "zktNzalf1POF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaner = DataCleaner()\n",
        "\n",
        "summarizer = Summarizer(\n",
        "    model_loader=model_loader,\n",
        "    data_cleaner=data_cleaner,\n",
        "    df=df,\n",
        "    max_allowed_tokens=100000,\n",
        "    summary_size=400\n",
        ")\n",
        "\n",
        "discriminative_evaluator = DiscriminativeEvaluator(df=df)\n",
        "# discriminative_evaluator = DiscriminativeEvaluator(df=df_mobility_impaired)\n",
        "\n",
        "\n",
        "submission_evaluator = SubmissionEvaluator(\n",
        "    model_loader=model_loader,\n",
        "    data_cleaner=data_cleaner,\n",
        "    df=df,\n",
        "    # df=df_mobility_impaired,\n",
        "    fewshot_df=fewshot_df,\n",
        "    fewshot_urls=fewshot_urls,\n",
        "    summarizer=summarizer,\n",
        "    output_length=output_length\n",
        ")\n",
        "\n",
        "contest_evaluator = ContestEvaluator(\n",
        "    submission_evaluator=submission_evaluator,\n",
        "    discriminative_evaluator=discriminative_evaluator,\n",
        "    summarizer=summarizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "NeBfCYRRu0KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Evaluate a Single Submission - Provide a contest_description manually\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# submission_url = \"https://www.hackster.io/kay-lerch/morse-coder-beep-beep-beeeeeep-lessons-ddd0dd\"\n",
        "# # contest_description = \"\"\"The Inclusive Innovation Challenge Ultimate Award is a contest that seeks to design assistive technologies that address the unique needs of individuals with disabilities. Participants are encouraged to submit solutions for three themes GAMING for people with mobility impairments TRAVELLING for people with mobility impairments and SWIMMING for people with visual impairments. The contest aims to revolutionize the assistive technologies landscape by actively involving individuals with disabilities in every stage of product design and prototyping. The ultimate goal is to drive innovation and shape a future where assistive technologies are truly groundbreaking and inclusive.\"\"\"\n",
        "# contest_description = \"The contest is about encouraging voice technology innovation with Alexa, a voice service that powers Amazon Echo, by submitting great skill ideas for an Alexa Skill Contest. The main goal is to create skills that demonstrate how voice allows users to interact with devices in innovative, fun, and useful ways, with no hardware required. Participants will address topics such as voice user interfaces, VUI best practices, and storytelling through their projects. The overall purpose of the contest is to showcase creativity and innovation in voice technology, with a focus on building and submitting Amazon Alexa Skills that can be published and used by users. The contest will be judged based on creativity, use of VUI best practices, storytelling, published skills, project documentation, code, and bonus points for using AWS Lambda. Participants are encouraged to build and submit multiple skills, and winners will be selected based on the judging criteria.\"\n",
        "\n",
        "generative_output, prompt = submission_evaluator.evaluate_submission(\n",
        "    submission_url,\n",
        "    contest_description\n",
        ")\n",
        "print(\"Generative Output:\\n\" + generative_output + \"\\n\")\n",
        "\n",
        "discriminative_output = discriminative_evaluator.discriminative_evaluation(submission_url)\n",
        "print(\"Discriminative Output:\\n\", discriminative_output)\n",
        "\n",
        "parsed_label = contest_evaluator.parse_llm_response(generative_output)\n",
        "print(\"Parsed Classification:\", parsed_label)\n",
        "\n",
        "print(\"Prompt:\\n\", prompt)"
      ],
      "metadata": {
        "id": "blkJC6EtBypE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # --------------------------------------------------------------\n",
        "# # # Evaluate a Single Submission - Produce the contest_description\n",
        "# # # --------------------------------------------------------------\n",
        "generative_output_auto, prompt = submission_evaluator.evaluate_submission(\n",
        "    submission_url,\n",
        "    contest_description=None\n",
        ")\n",
        "print(\"Generative Output (auto contest_description):\\n\", generative_output_auto)"
      ],
      "metadata": {
        "id": "0j7XSTX2kxRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title\n",
        "# # -------------------------------------\n",
        "# # Evaluate a Full Contest\n",
        "# # -------------------------------------\n",
        "\n",
        "# # Alexa\n",
        "# contest_name = \"amazon-alexa-skill-contest-one\"\n",
        "# summary_text = \"The contest is about encouraging voice technology innovation with Alexa, a voice service that powers Amazon Echo, by submitting great skill ideas for an Alexa Skill Contest. The main goal is to create skills that demonstrate how voice allows users to interact with devices in innovative, fun, and useful ways, with no hardware required. Participants will address topics such as voice user interfaces, VUI best practices, and storytelling through their projects. The overall purpose of the contest is to showcase creativity and innovation in voice technology, with a focus on building and submitting Amazon Alexa Skills that can be published and used by users. The contest will be judged based on creativity, use of VUI best practices, storytelling, published skills, project documentation, code, and bonus points for using AWS Lambda. Participants are encouraged to build and submit multiple skills, and winners will be selected based on the judging criteria.\"\n",
        "\n",
        "# # Buildtogether\n",
        "# # contest_name = \"buildtogether\"\n",
        "# # summary_text = \"\"\"The Inclusive Innovation Challenge Ultimate Award is a contest that seeks to design assistive technologies that address the unique needs of individuals with disabilities. Participants are encouraged to submit solutions for three themes GAMING for people with mobility impairments TRAVELLING for people with mobility impairments and SWIMMING for people with visual impairments. The contest aims to revolutionize the assistive technologies landscape by actively involving individuals with disabilities in every stage of product design and prototyping. The ultimate goal is to drive innovation and shape a future where assistive technologies are truly groundbreaking and inclusive.\"\"\"\n",
        "\n",
        "# # contest_name = \"buildtogether2\"\n",
        "# # summary_text = \"\"\"The contest is an innovation challenge focused on developing assistive technologies to improve the lives of individuals with mobility impairments. Participants are tasked with identifying gaps in existing solutions and designing novel and feasible solutions. Ultimately, the contest aims to advance assistive technologies and drive innovation toward a more inclusive world for individuals with mobility impairments.\"\"\"\n",
        "\n",
        "\n",
        "results_df = contest_evaluator.evaluate_full_contest(\n",
        "    contest_name=contest_name,\n",
        "    df=df,\n",
        "    output_csv_path=f\"amazon-alexa_evaluation_14_03.csv\",\n",
        "    contest_description=summary_text\n",
        ")\n",
        "\n",
        "\n",
        "# Automatically download the CSV file\n",
        "results_df.to_csv(f\"evaluation_{contest_name}.csv\", index=False)\n",
        "files.download(f\"evaluation_{contest_name}.csv\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oi-bErHWaAw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# Evaluate a Multiple Contest\n",
        "# -------------------------------------\n",
        "\n",
        "unique_contests = [\n",
        "    \"onion\",\n",
        "    \"UNDPCOVID19\",\n",
        "    \"amd2023\",\n",
        "  # put whatever contests here you want to evaluate\n",
        "]\n",
        "\n",
        "total_contests = len(unique_contests)\n",
        "\n",
        "# Define output file for merged results\n",
        "merged_csv_path = \"evaluation_covid-china.csv\"\n",
        "all_results = []  # List to store DataFrames\n",
        "output_path = \"/content/drive/MyDrive/Master_Thesis/Final Evaluation\"\n",
        "\n",
        "\n",
        "\n",
        "# Loop through only the filtered contests\n",
        "for index, contest_name in enumerate(unique_contests, start=1):\n",
        "    results_df = contest_evaluator.evaluate_full_contest(\n",
        "        contest_name=contest_name,\n",
        "        df=df,\n",
        "        output_csv_path=f\"{output_path}/{contest_name}_18_03.csv\",\n",
        "    )\n",
        "\n",
        "    # Append results to list\n",
        "    all_results.append(results_df)\n",
        "\n",
        "    # Save intermediate results every 12 contests\n",
        "    if index % 12 == 0:  # Save every 12 contests\n",
        "        partial_results_df = pd.concat(all_results, ignore_index=True)\n",
        "        partial_csv_path = f\"{output_path}/evaluation_nvidia-ada_part_{index}.csv\"\n",
        "        partial_results_df.to_csv(partial_csv_path, index=False)\n",
        "        print(f\"\\nSaved partial results: {partial_csv_path}\")\n",
        "        files.download(partial_csv_path)  # Download intermediate result\n",
        "\n",
        "    # Print progress update\n",
        "    print(f\"Processed {index}/{total_contests} contests: {contest_name}\")\n",
        "\n",
        "# Final merge and save\n",
        "final_results_df = pd.concat(all_results, ignore_index=True)\n",
        "final_results_df.to_csv(merged_csv_path, index=False)\n",
        "print(f\"\\nAll contest results merged and saved to: {merged_csv_path}\")\n",
        "files.download(merged_csv_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "79j29NpIGoZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contest_name = \"arduino-microsoft-maker\"\n",
        "# contest_name = \"buildtogether2\"\n",
        "model_name = \"Llama 3.1 8B\"\n",
        "# model_name = \"DeepSeek R1 Distill Qwen 8B\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plotter = ContestSummaryPlotter(\n",
        "    # csv_path=os.path.join(input_path, f\"evaluation_{contest_name}.csv\"),\n",
        "\n",
        "    csv_path=os.path.join(\"/content\", f\"evaluation_{contest_name}_17_3.csv\"),\n",
        "\n",
        "    contest_name=contest_name,\n",
        "    model_name=model_name\n",
        ")\n",
        "\n",
        "# Generate summary and plots\n",
        "plotter.generate_summary_and_plot()"
      ],
      "metadata": {
        "id": "L9RAcFtTaI6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contest_name = \"amazon-alexa-skill-contest-one\"\n",
        "# model_name = \"DeepSeek-R1-Distill-Llama-8B\"\n",
        "# contest_evaluation_path = os.path.join(input_path, f\"DeepSeek_evaluation_{contest_name}.csv\")\n",
        "\n",
        "# contest_evaluator.generate_summary_and_plot(contest_evaluation_path, contest_name, model_name)"
      ],
      "metadata": {
        "id": "WNWDxxaJ7mGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# # Define the file paths\n",
        "# file1 = \"/content/evaluation_amazon-alexa-skill-contest-one.csv\"\n",
        "# file2 = \"/content/drive/MyDrive/Master_Thesis/LLM_Input/Llama_evaluation_amazon-alexa-skill-contest-one.csv\"\n",
        "\n",
        "# # Load the CSVs into DataFrames\n",
        "# df1 = pd.read_csv(file1)\n",
        "# df2 = pd.read_csv(file2)\n",
        "\n",
        "# # Concatenate both DataFrames\n",
        "# merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# # Save the merged DataFrame to a new CSV\n",
        "# output_file = \"/content/merged_evaluation_amazon-alexa-skill-contest-one.csv\"\n",
        "# merged_df.to_csv(output_file, index=False)\n",
        "\n",
        "# print(f\"Merged CSV saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "-aJKNJ9Rezn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize qs script\n",
        "\n",
        "# # Update \"num_of_submissions\" to total number of entries\n",
        "# num_entries = len(df_mobility_impaired)\n",
        "# df_mobility_impaired[\"num_of_submissions\"] = num_entries\n",
        "\n",
        "# # Normalize \"qs_normalized\" between 0 and 1 (rounded to 3 decimal places)\n",
        "# df_mobility_impaired[\"qs_normalized\"] = ((df_mobility_impaired[\"qs_normalized\"] - df_mobility_impaired[\"qs_normalized\"].min()) /\n",
        "#                                          (df_mobility_impaired[\"qs_normalized\"].max() - df_mobility_impaired[\"qs_normalized\"].min())).round(3)\n",
        "\n",
        "# # Shuffle rows to randomize ranking of tied values\n",
        "# df_mobility_impaired = df_mobility_impaired.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# # Assign rank (highest \"qs_normalized\" gets rank 1)\n",
        "# df_mobility_impaired[\"qs_rank\"] = df_mobility_impaired[\"qs_normalized\"].rank(method=\"first\", ascending=False).astype(int)\n",
        "\n",
        "# # Save the updated dataset\n",
        "# output_path = os.path.join(input_path, 'mobility_impaired_submissions_updated.csv')\n",
        "# df_mobility_impaired.to_csv(output_path, index=False)\n",
        "\n",
        "# print(f\"Updated dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "euOeuUzGosFT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}