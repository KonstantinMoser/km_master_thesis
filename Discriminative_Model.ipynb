{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install lightning"
      ],
      "metadata": {
        "id": "QJatWBAMXMOS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
        "from sklearn.metrics import recall_score, make_scorer, confusion_matrix, accuracy_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tpH-3RlZQnF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DriveManager\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DriveManager\n",
        "# --------------------------------------------------\n",
        "class DriveManager:\n",
        "    \"\"\"\n",
        "    Class Name: DriveManager\n",
        "\n",
        "    Purpose:\n",
        "        - Handle mounting Google Drive in a Colab environment.\n",
        "        - Provide convenient path handling if needed.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Mount/unmount Google Drive.\n",
        "        - Potentially provide standard path resolutions for loading/saving.\n",
        "\n",
        "    Example Usage:\n",
        "        drive_mgr = DriveManager()\n",
        "        drive_mgr.mount_drive()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mount_path=\"/content/drive/\"):\n",
        "        \"\"\"\n",
        "        Constructor for DriveManager.\n",
        "\n",
        "        Args:\n",
        "            mount_path (str): The path at which to mount Google Drive.\n",
        "\n",
        "        Attributes:\n",
        "            mount_path (str): Where Google Drive is mounted in Colab.\n",
        "        \"\"\"\n",
        "        self.mount_path = mount_path\n",
        "\n",
        "    def mount_drive(self, force_remount=True):\n",
        "        \"\"\"\n",
        "        Mounts Google Drive using the provided mount path.\n",
        "\n",
        "        Args:\n",
        "            force_remount (bool): Whether to force-remount if already mounted.\n",
        "        \"\"\"\n",
        "        from google.colab import drive\n",
        "        drive.mount(self.mount_path, force_remount=force_remount)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-VaqmbruYy3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CSVDataset\n",
        "\n",
        "# --------------------------------------------------\n",
        "# CSVDataset\n",
        "# --------------------------------------------------\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Class Name: CSVDataset\n",
        "\n",
        "    Purpose:\n",
        "        - Custom PyTorch Dataset to load features and labels from a Pandas DataFrame.\n",
        "        - Optionally apply SMOTE oversampling on the data to handle imbalance.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Load data from a DataFrame, separate features and labels.\n",
        "        - Normalize continuous features.\n",
        "        - Optionally perform SMOTE oversampling to balance classes.\n",
        "\n",
        "    Example Usage:\n",
        "        dataset = CSVDataset(train_df, smote=True)\n",
        "        data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, smote=True):\n",
        "        \"\"\"\n",
        "        Constructor for CSVDataset.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing the data.\n",
        "            smote (bool): If True, applies SMOTE oversampling.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        x = df[['submission_word_count', 'num_image', 'num_gif', 'num_video', 'video_duration',\n",
        "                'num_things', 'cad', 'schematic', 'code', 'code_lines', 'link']]\n",
        "\n",
        "        # Normalize numeric features\n",
        "        self.x = self.normalize_data(x).values\n",
        "        self.y = df['winner'].values\n",
        "\n",
        "        if smote:\n",
        "            self.smote()\n",
        "\n",
        "    def smote(self):\n",
        "        \"\"\"\n",
        "        Applies SMOTE oversampling to the dataset to handle class imbalance.\n",
        "        \"\"\"\n",
        "        sm = SMOTE(random_state=24)\n",
        "        self.x, self.y = sm.fit_resample(self.x, self.y)\n",
        "\n",
        "    def normalize_data(self, df_):\n",
        "        \"\"\"\n",
        "        Normalizes numeric columns to [0, 1] range, ignoring binary columns.\n",
        "\n",
        "        Args:\n",
        "            df_ (pd.DataFrame): DataFrame of features to normalize.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The normalized DataFrame.\n",
        "        \"\"\"\n",
        "        df = df_.copy()\n",
        "        for column in df.columns:\n",
        "            if df[column].dtype in ['int64', 'float64']:\n",
        "                # Skip columns that are purely 0 or 1\n",
        "                if not all(x in (0, 1) for x in df[column]):\n",
        "                    min_val = df[column].min()\n",
        "                    max_val = df[column].max()\n",
        "                    df[column] = (df[column] - min_val) / (max_val - min_val)\n",
        "        return df\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples.\n",
        "        \"\"\"\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns one sample of data.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple(torch.Tensor, torch.Tensor): (features, label)\n",
        "        \"\"\"\n",
        "        return torch.tensor(self.x[idx], dtype=torch.float), torch.tensor(self.y[idx], dtype=torch.float)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "5inGT2xXQnDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SimpleMLP, LogisticRegressionModel, OptimizedModelWrapper\n",
        "\n",
        "# --------------------------------------------------\n",
        "# SimpleMLP\n",
        "# --------------------------------------------------\n",
        "class SimpleMLP(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Class Name: SimpleMLP\n",
        "\n",
        "    Purpose:\n",
        "        - A simple Multilayer Perceptron (MLP) model built with PyTorch Lightning.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Define a forward pass through a 3-layer fully-connected neural network.\n",
        "        - Handle training/validation steps and loss computation using BCE.\n",
        "\n",
        "    Example Usage:\n",
        "        model = SimpleMLP(input_size=11, hidden_size=32, output_size=1)\n",
        "        trainer = pl.Trainer(max_epochs=10)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Constructor for SimpleMLP.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features.\n",
        "            hidden_size (int): Number of units in hidden layers.\n",
        "            output_size (int): Number of output units (1 for binary classification).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Sigmoid output (probability of class 1).\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Training step: computes binary cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): (x, y) data from the DataLoader.\n",
        "            batch_idx (int): Batch index (not used).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Training loss.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y_hat = self(x).squeeze()\n",
        "        y = y.float()\n",
        "        loss = F.binary_cross_entropy(y_hat, y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Validation step: computes binary cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): (x, y) data from the DataLoader.\n",
        "            batch_idx (int): Batch index (not used).\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y_hat = self(x).squeeze()\n",
        "        y = y.float()\n",
        "        loss = F.binary_cross_entropy(y_hat, y)\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the Adam optimizer with a fixed learning rate.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# LogisticRegressionModel\n",
        "# --------------------------------------------------\n",
        "class LogisticRegressionModel(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Class Name: LogisticRegressionModel\n",
        "\n",
        "    Purpose:\n",
        "        - A logistic regression model wrapped in PyTorch Lightning.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Define a single linear layer for logistic regression.\n",
        "        - Handle training/validation steps and loss computation using BCE.\n",
        "\n",
        "    Example Usage:\n",
        "        model = LogisticRegressionModel(input_size=11)\n",
        "        trainer = pl.Trainer(max_epochs=10)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        \"\"\"\n",
        "        Constructor for LogisticRegressionModel.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Number of input features.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for logistic regression.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Sigmoid output.\n",
        "        \"\"\"\n",
        "        logits = self.linear(x)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs.squeeze()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Training step: computes binary cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): (x, y) data from the DataLoader.\n",
        "            batch_idx (int): Batch index (not used).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Training loss.\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        y = y.float()\n",
        "        loss = F.binary_cross_entropy(y_hat, y)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Validation step: computes binary cross-entropy loss.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): (x, y) data from the DataLoader.\n",
        "            batch_idx (int): Batch index (not used).\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        y = y.float()\n",
        "        loss = F.binary_cross_entropy(y_hat, y)\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the Adam optimizer with a fixed learning rate.\n",
        "        \"\"\"\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# OptimizedModelWrapper\n",
        "# --------------------------------------------------\n",
        "class OptimizedModelWrapper(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Class Name: OptimizedModelWrapper\n",
        "\n",
        "    Purpose:\n",
        "        - A wrapper that trains various scikit-learn models (RandomForest, XGBoost, SVM, LogisticRegression)\n",
        "          and performs hyperparameter optimization (via GridSearchCV where applicable).\n",
        "\n",
        "    Responsibilities:\n",
        "        - Select model based on provided model_name.\n",
        "        - Perform grid search (where applicable).\n",
        "        - For XGBoost, handle early stopping via eval set.\n",
        "        - Provide a forward method to integrate with PyTorch Lightning workflow.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name, train_X, train_y, val_X=None, val_y=None):\n",
        "        \"\"\"\n",
        "        Constructor for OptimizedModelWrapper.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model to train (RandomForest, XGBoost, SVM, LogisticRegression).\n",
        "            train_X (numpy.ndarray): Training features.\n",
        "            train_y (numpy.ndarray): Training labels.\n",
        "            val_X (numpy.ndarray, optional): Validation features (required for XGBoost).\n",
        "            val_y (numpy.ndarray, optional): Validation labels (required for XGBoost).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.train_X = train_X\n",
        "        self.train_y = train_y\n",
        "        self.val_X = val_X\n",
        "        self.val_y = val_y\n",
        "        self.model = self._get_optimized_model()\n",
        "\n",
        "    def _get_optimized_model(self):\n",
        "        \"\"\"\n",
        "        Internal method: Defines and performs the required optimization for the selected model.\n",
        "\n",
        "        Returns:\n",
        "            sklearn model: Trained and optimized model.\n",
        "        \"\"\"\n",
        "        if self.model_name == \"RandomForest\":\n",
        "            param_grid = {\n",
        "                'n_estimators': [100],\n",
        "                'max_depth': [3],\n",
        "                'min_samples_split': [10]\n",
        "            }\n",
        "            model = RandomForestClassifier()\n",
        "\n",
        "        elif self.model_name == \"XGBoost\":\n",
        "            if self.val_X is None or self.val_y is None:\n",
        "                raise ValueError(\"Validation data is required for XGBoost with early stopping.\")\n",
        "            model = XGBClassifier(\n",
        "                use_label_encoder=False,\n",
        "                eval_metric='logloss',\n",
        "                max_depth=3,\n",
        "                n_estimators=50,\n",
        "                learning_rate=0.05,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8\n",
        "            )\n",
        "            model.fit(\n",
        "                self.train_X,\n",
        "                self.train_y,\n",
        "                eval_set=[(self.val_X, self.val_y)],\n",
        "                verbose=True  # Set to False if you want to suppress the training output\n",
        "            )\n",
        "            return model\n",
        "\n",
        "        elif self.model_name == \"SVM\":\n",
        "            param_grid = {\n",
        "                'C': [0.1],\n",
        "                'kernel': ['linear']\n",
        "            }\n",
        "            model = SVC(probability=True)\n",
        "\n",
        "        elif self.model_name == \"LogisticRegression\":\n",
        "            param_grid = {\n",
        "                'C': [0.1],\n",
        "                'penalty': ['l2']\n",
        "            }\n",
        "            model = LogisticRegression(max_iter=500)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid model name. Choose from: RandomForest, XGBoost, SVM, LogisticRegression\")\n",
        "\n",
        "        # For non-XGBoost models, perform Grid Search\n",
        "        scorer = make_scorer(recall_score)\n",
        "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring=scorer)\n",
        "        grid_search.fit(self.train_X, self.train_y)\n",
        "        print(f\"Best Parameters for {self.model_name}: {grid_search.best_params_}\")\n",
        "        print(f\"Best Cross-Validation Recall: {grid_search.best_score_}\")\n",
        "        return grid_search.best_estimator_\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Placeholder method for validation in a PyTorch Lightning loop.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): (features, labels).\n",
        "            batch_idx (int): Batch index (not used).\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        x, y = x.cpu(), y.cpu()\n",
        "        preds = self.forward(x)\n",
        "        loss = F.binary_cross_entropy(preds, y.float())\n",
        "        self.log(\"val_loss\", loss)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward method to integrate with PyTorch Lightning.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input features.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predictions as probabilities for class 1.\n",
        "        \"\"\"\n",
        "        x_np = x.cpu().numpy()\n",
        "        probs = self.model.predict_proba(x_np)[:, 1]\n",
        "        return torch.tensor(probs, dtype=torch.float32)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        No optimizer is used here, as training occurs via scikit-learn's methods.\n",
        "        \"\"\"\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "v-uXN03fRQ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DiscriminativeModel\n",
        "\n",
        "# --------------------------------------------------\n",
        "# DiscriminativeModel\n",
        "# --------------------------------------------------\n",
        "class DiscriminativeModel:\n",
        "    \"\"\"\n",
        "    Class Name: DiscriminativeModel\n",
        "\n",
        "    Purpose:\n",
        "        - Orchestrate model training (across multiple runs/folds) and evaluate performance.\n",
        "        - Provide a high-level interface to:\n",
        "            1. Split data into train/val/test sets.\n",
        "            2. Train either PyTorch Lightning models (e.g., SimpleMLP) or scikit-learn models\n",
        "               (wrapped in OptimizedModelWrapper).\n",
        "            3. Evaluate on a test set using a submission-based recall metric.\n",
        "            4. (Optionally) Compute and save false negatives, all submissions with model-generated scores,\n",
        "               and summary metrics (Accuracy, Recall, TN, FP, FN, TP).\n",
        "\n",
        "    Responsibilities:\n",
        "        - Manage K-Fold logic, train/val/test splitting, and repeated runs.\n",
        "        - Contain a method to perform evaluation on a given test set (`submission_evaluation`).\n",
        "        - Contain a method (`run_evaluation`) that orchestrates multiple runs/folds and then\n",
        "          aggregates fold-level metrics into a final run-level table.\n",
        "\n",
        "    Example Usage:\n",
        "        discriminative_model = DiscriminativeModel(df, output_path=\"/content/drive/MyDrive/...\")\n",
        "        discriminative_model.run_evaluation(n_runs=5, n_folds=5, model_type=\"LogisticRegression\", dataset_name=\"28_11.csv\")\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, output_path):\n",
        "        \"\"\"\n",
        "        Constructor for DiscriminativeModel.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): The full dataset.\n",
        "            output_path (str): Where to store evaluation CSVs.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.output_path = output_path\n",
        "\n",
        "    def submission_evaluation(\n",
        "        self,\n",
        "        model,\n",
        "        test_df,\n",
        "        run,\n",
        "        fold,\n",
        "        dataset_name,\n",
        "        model_type,\n",
        "        save_contest_results=False,\n",
        "        save_false_negatives=False,\n",
        "        save_all_submissions=False,\n",
        "        compute_additional_metrics=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model on the test set by filtering out various percentages of submissions.\n",
        "        Optionally save false negatives and all submissions with model scores.\n",
        "        Optionally compute additional per-filter metrics (Accuracy, Recall, TN, FP, FN, TP).\n",
        "\n",
        "        Args:\n",
        "            model: The trained model (could be PyTorch Lightning or sklearn).\n",
        "            test_df (pd.DataFrame): DataFrame containing the test data.\n",
        "            run (int): Current run number.\n",
        "            fold (int): Current fold index.\n",
        "            dataset_name (str): Name of the dataset (used for saving results).\n",
        "            model_type (str): Type of the model ('MLP', 'LogisticRegression', 'RandomForest', 'XGBoost', 'SVM').\n",
        "            save_contest_results (bool): If True, save per-contest results to CSV.\n",
        "            save_false_negatives (bool): If True, save the false negatives to CSV.\n",
        "            save_all_submissions (bool): If True, save all submissions with scores to CSV.\n",
        "            compute_additional_metrics (bool): If True, compute & summarize Accuracy, Recall, TN, FP, FN, TP for each filter.\n",
        "\n",
        "        Returns:\n",
        "            (df_results, accuracy, overall_recall, conf_matrix, metrics_summary_df, fold_metrics_data):\n",
        "                - df_results (pd.DataFrame): Per-contest results with recall at each filter percentage.\n",
        "                - accuracy (float or None): Overall accuracy at 50% filtering (or None if no data).\n",
        "                - overall_recall (float or None): Overall recall at 50% filtering (or None if no data).\n",
        "                - conf_matrix (np.array or None): Confusion matrix at 50% filtering (or None if no data).\n",
        "                - metrics_summary_df (pd.DataFrame or None): Additional metrics at each filter (this fold).\n",
        "                - fold_metrics_data (dict): Raw fold-level sums/averages that can be aggregated across folds.\n",
        "        \"\"\"\n",
        "        print(f\"----- Testset Evaluation by filtering out varying percentages of submissions for each contest -----\")\n",
        "        filter_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "\n",
        "        # Lists/dicts to store results\n",
        "        per_contest_results = []\n",
        "        false_negatives = []\n",
        "        all_submissions = []\n",
        "\n",
        "        # We will keep track of predictions for each filter percentage across all contests\n",
        "        # in order to compute additional metrics if requested.\n",
        "        metrics_dict = {\n",
        "            p: {\"all_preds\": [], \"all_labels\": []}\n",
        "            for p in filter_percentages\n",
        "        }\n",
        "\n",
        "        # Group the test set by 'contest_name'\n",
        "        for contest_name, contest_data in test_df.groupby('contest_name'):\n",
        "\n",
        "            # Extract the 'prizes_sum' information for the contest\n",
        "            prizes_sum = contest_data['prizes_sum'].iloc[0] if 'prizes_sum' in contest_data.columns else None\n",
        "\n",
        "            # Convert into dataset\n",
        "            dataset = CSVDataset(contest_data, smote=False)\n",
        "            x = torch.tensor(dataset.x, dtype=torch.float)\n",
        "            y = torch.tensor(dataset.y, dtype=torch.float)\n",
        "\n",
        "            total_submissions = len(y)\n",
        "            total_winners = int(y.sum().item())\n",
        "\n",
        "            if total_submissions == 0:\n",
        "                continue\n",
        "\n",
        "            contest_info = {\n",
        "                'contest_name': contest_name,\n",
        "                'prizes_sum': prizes_sum,\n",
        "                'nr_winners': total_winners,\n",
        "                'nr_submissions': total_submissions\n",
        "            }\n",
        "\n",
        "            # Generate predictions\n",
        "            with torch.no_grad():\n",
        "                if model_type == \"MLP\":\n",
        "                    # MLP (PyTorch Lightning) forward\n",
        "                    y_hat = model(x).squeeze()\n",
        "                else:\n",
        "                    # sklearn model or wrapper\n",
        "                    if hasattr(model, \"predict_proba\"):\n",
        "                        y_hat = torch.tensor(model.predict_proba(x.numpy())[:, 1])\n",
        "                    else:\n",
        "                        y_hat = torch.tensor(model.predict(x.numpy()))\n",
        "\n",
        "            y_hat_flat = y_hat.flatten()\n",
        "            sorted_probs, sorted_indices = torch.sort(y_hat_flat, descending=True)\n",
        "\n",
        "            # Store quality scores for all submissions (for optional saving)\n",
        "            for idx, submission in enumerate(contest_data.itertuples(index=False)):\n",
        "                submission_info = submission._asdict()\n",
        "                submission_info['quality_score'] = y_hat[idx].item()\n",
        "                submission_info['model'] = fold + 1\n",
        "                all_submissions.append(submission_info)\n",
        "\n",
        "            # For each filter percentage, compute recall and store predictions\n",
        "            for percentage in filter_percentages:\n",
        "                keep_percentage = 1 - percentage\n",
        "                top_n = int(keep_percentage * total_submissions)\n",
        "                top_n = max(top_n, 1)\n",
        "\n",
        "                top_indices = sorted_indices[:top_n]\n",
        "                preds = torch.zeros_like(y_hat)\n",
        "                preds[top_indices] = 1\n",
        "\n",
        "                # Per-contest recall\n",
        "                recall = recall_score(y.cpu().numpy(), preds.cpu().numpy())\n",
        "                percentage_label = f'recall_{int(percentage*100)}%'\n",
        "                contest_info[percentage_label] = recall\n",
        "\n",
        "                # Accumulate global predictions if we want additional metrics\n",
        "                if compute_additional_metrics:\n",
        "                    metrics_dict[percentage][\"all_preds\"].append(preds)\n",
        "                    metrics_dict[percentage][\"all_labels\"].append(y)\n",
        "\n",
        "                # Track false negatives specifically at 50% filtering, or we can do\n",
        "                # it for each percentage if desired. Here, we mirror your original logic:\n",
        "                if abs(percentage - 0.5) < 1e-9:\n",
        "                    # Identify false negatives\n",
        "                    for idx, (true_label, pred_label) in enumerate(zip(y, preds)):\n",
        "                        if true_label == 1 and pred_label == 0:\n",
        "                            fn_info = contest_data.iloc[idx].to_dict()\n",
        "                            fn_info['quality_score'] = y_hat[idx].item()\n",
        "                            fn_info['model'] = fold + 1\n",
        "                            false_negatives.append(fn_info)\n",
        "\n",
        "            per_contest_results.append(contest_info)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Compute metrics at 50% filtering (fold-level)\n",
        "        # -----------------------------\n",
        "        p_50 = 0.5\n",
        "        if len(metrics_dict[p_50][\"all_preds\"]) > 0:\n",
        "            all_preds_50 = torch.cat(metrics_dict[p_50][\"all_preds\"]).cpu().numpy()\n",
        "            all_labels_50 = torch.cat(metrics_dict[p_50][\"all_labels\"]).cpu().numpy()\n",
        "\n",
        "            accuracy = accuracy_score(all_labels_50, all_preds_50)\n",
        "            overall_recall = recall_score(all_labels_50, all_preds_50)\n",
        "            conf_matrix = confusion_matrix(all_labels_50, all_preds_50)\n",
        "\n",
        "            print(\"\\n----- Overall Evaluation at 50% filtering (FOLD LEVEL) -----\")\n",
        "            print(f\"Fold {fold+1} accuracy: {accuracy}\")\n",
        "            print(f\"Fold {fold+1} recall: {overall_recall}\")\n",
        "            print(f\"Fold {fold+1} Confusion Matrix:\\n{conf_matrix}\")\n",
        "            print(f\"Fold {fold+1} Remaining % size of Submissions: {(sum(all_preds_50) / len(all_preds_50)) * 100:.2f}%\")\n",
        "        else:\n",
        "            accuracy = None\n",
        "            overall_recall = None\n",
        "            conf_matrix = None\n",
        "\n",
        "        df_results = pd.DataFrame(per_contest_results)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Save per-contest results CSV\n",
        "        # -----------------------------\n",
        "        if save_contest_results:\n",
        "            output_csv = os.path.join(self.output_path, f'{model_type}_results_{dataset_name[:-4]}_run{run}.csv')\n",
        "            file_exists = os.path.exists(output_csv)\n",
        "            df_results.to_csv(output_csv, mode='a', header=not file_exists, index=False)\n",
        "            print(f\"\\nPer-contest results (fold-level) saved to {output_csv}\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Save false negatives if requested\n",
        "        # -----------------------------\n",
        "        if save_false_negatives:\n",
        "            if false_negatives:\n",
        "                false_negatives_df = pd.DataFrame(false_negatives)\n",
        "                false_negatives_csv = os.path.join(self.output_path, f'{model_type}_fn_{dataset_name[:-4]}_run{run}.csv')\n",
        "                file_exists = os.path.exists(false_negatives_csv)\n",
        "                false_negatives_df.to_csv(false_negatives_csv, mode='a', header=not file_exists, index=False)\n",
        "                print(f\"False negatives saved to {false_negatives_csv}\")\n",
        "            else:\n",
        "                print(\"No false negatives found at 50% filtering in this fold.\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Save all submissions if requested\n",
        "        # -----------------------------\n",
        "        if save_all_submissions:\n",
        "            if all_submissions:\n",
        "                all_submissions_df = pd.DataFrame(all_submissions)\n",
        "                all_submissions_csv = os.path.join(self.output_path, f'{model_type}_all_submissions_{dataset_name[:-4]}_run{run}.csv')\n",
        "                file_exists = os.path.exists(all_submissions_csv)\n",
        "                all_submissions_df.to_csv(all_submissions_csv, mode='a', header=not file_exists, index=False)\n",
        "                print(f\"All submissions with quality scores saved to {all_submissions_csv}\")\n",
        "            else:\n",
        "                print(\"No submissions found to save for this fold.\")\n",
        "\n",
        "        # -----------------------------\n",
        "        # Compute & return additional metrics across all filter percentages (fold-level)\n",
        "        # -----------------------------\n",
        "        metrics_summary_df = None\n",
        "        fold_metrics_data = {}  # We'll collect raw sums for tn, fp, fn, tp, plus lists to average.\n",
        "\n",
        "        if compute_additional_metrics:\n",
        "            all_metrics_rows = []\n",
        "            for p in sorted(filter_percentages):\n",
        "                preds_list = metrics_dict[p][\"all_preds\"]\n",
        "                labels_list = metrics_dict[p][\"all_labels\"]\n",
        "                if len(preds_list) == 0:\n",
        "                    continue\n",
        "\n",
        "                all_preds_p = torch.cat(preds_list).cpu().numpy()\n",
        "                all_labels_p = torch.cat(labels_list).cpu().numpy()\n",
        "\n",
        "                # confusion matrix\n",
        "                tn, fp, fn, tp = confusion_matrix(all_labels_p, all_preds_p).ravel()\n",
        "                accuracy_p = (tp + tn) / (tp + tn + fp + fn)\n",
        "                recall_p = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "                remain_subs_pct = (sum(all_preds_p) / len(all_preds_p)) * 100\n",
        "\n",
        "                row = {\n",
        "                    \"Filter_Percentage\": p,\n",
        "                    \"Remaining_Percentage_Size_of_Submissions\": remain_subs_pct,\n",
        "                    \"Accuracy\": accuracy_p,\n",
        "                    \"Recall\": recall_p,\n",
        "                    \"True_Negatives\": tn,\n",
        "                    \"False_Positives\": fp,\n",
        "                    \"False_Negatives\": fn,\n",
        "                    \"True_Positives\": tp\n",
        "                }\n",
        "                all_metrics_rows.append(row)\n",
        "\n",
        "            metrics_summary_df = pd.DataFrame(all_metrics_rows)\n",
        "            print(\"\\nFold-Level Additional Metrics (All Contests Combined) for each Filter %:\")\n",
        "            print(metrics_summary_df)\n",
        "\n",
        "            # Prepare the fold_metrics_data to later be aggregated in run_evaluation\n",
        "            # We'll store sums for TN, FP, FN, TP, and lists for accuracy, recall, remain_subs_pct\n",
        "            fold_metrics_data = {\n",
        "                p: {\n",
        "                    \"tn\": 0, \"fp\": 0, \"fn\": 0, \"tp\": 0,\n",
        "                    \"acc_list\": [], \"recall_list\": [], \"remain_pct_list\": []\n",
        "                }\n",
        "                for p in sorted(filter_percentages)\n",
        "            }\n",
        "\n",
        "            for _, row in metrics_summary_df.iterrows():\n",
        "                p = row[\"Filter_Percentage\"]\n",
        "                fold_metrics_data[p][\"tn\"] += row[\"True_Negatives\"]\n",
        "                fold_metrics_data[p][\"fp\"] += row[\"False_Positives\"]\n",
        "                fold_metrics_data[p][\"fn\"] += row[\"False_Negatives\"]\n",
        "                fold_metrics_data[p][\"tp\"] += row[\"True_Positives\"]\n",
        "                fold_metrics_data[p][\"acc_list\"].append(row[\"Accuracy\"])\n",
        "                fold_metrics_data[p][\"recall_list\"].append(row[\"Recall\"])\n",
        "                fold_metrics_data[p][\"remain_pct_list\"].append(row[\"Remaining_Percentage_Size_of_Submissions\"])\n",
        "\n",
        "        return (\n",
        "            df_results,\n",
        "            accuracy,\n",
        "            overall_recall,\n",
        "            conf_matrix,\n",
        "            metrics_summary_df,\n",
        "            fold_metrics_data  # raw sums/lists for each filter\n",
        "        )\n",
        "\n",
        "    def run_evaluation(\n",
        "        self,\n",
        "        n_runs,\n",
        "        n_folds,\n",
        "        model_type,\n",
        "        dataset_name,\n",
        "        mlp_hidden_size=32,\n",
        "        mlp_epochs=20,\n",
        "        save_contest_results=False,\n",
        "        save_false_negatives=False,\n",
        "        save_all_submissions=False,\n",
        "        compute_additional_metrics=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the training & evaluation loop multiple times (n_runs),\n",
        "        each time with K-Fold splitting (n_folds).\n",
        "\n",
        "        After each run, we aggregate fold-level metrics to produce a\n",
        "        run-level summary table of filter percentages vs.\n",
        "        (avg) Accuracy, (avg) Recall, (avg) Remaining%, (sum) TN/FP/FN/TP.\n",
        "\n",
        "        Args:\n",
        "            n_runs (int): Number of times to run the cross-validation process.\n",
        "            n_folds (int): Number of folds to use in K-Fold.\n",
        "            model_type (str): Type of model to train ('MLP', 'LogisticRegression', etc.).\n",
        "            dataset_name (str): Name of the dataset (used for saving results).\n",
        "            mlp_hidden_size (int): Hidden layer size for MLP (if MLP is used).\n",
        "            mlp_epochs (int): Number of epochs for MLP training (if MLP is used).\n",
        "            save_contest_results (bool): If True, save per-contest results to CSV.\n",
        "            save_false_negatives (bool): If True, saves false negatives to CSV in `submission_evaluation`.\n",
        "            save_all_submissions (bool): If True, saves all submissions to CSV in `submission_evaluation`.\n",
        "            compute_additional_metrics (bool): If True, compute & print Accuracy, Recall, and confusion matrix values\n",
        "                                               for each filter percentage, plus final run-level table.\n",
        "        \"\"\"\n",
        "        from sklearn.model_selection import KFold, train_test_split\n",
        "        import numpy as np\n",
        "\n",
        "        # Unique contest names\n",
        "        contest_names = self.df['contest_name'].unique()\n",
        "\n",
        "        # For printing final average recall at 50% filter across folds\n",
        "        recall_per_fold = []\n",
        "\n",
        "        # Predefine the filter percentages we use\n",
        "        filter_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "\n",
        "        for run in range(1, n_runs + 1):\n",
        "            print(f\"\\n========== STARTING RUN {run}/{n_runs} for {model_type} ==========\")\n",
        "\n",
        "            # We'll accumulate fold-level metrics into run_metrics_dict, so we can produce a final summary\n",
        "            run_metrics_dict = {\n",
        "                p: {\n",
        "                    \"tn\": 0, \"fp\": 0, \"fn\": 0, \"tp\": 0,\n",
        "                    \"acc_list\": [], \"recall_list\": [], \"remain_pct_list\": []\n",
        "                }\n",
        "                for p in filter_percentages\n",
        "            }\n",
        "\n",
        "            kf = KFold(n_splits=n_folds, shuffle=True, random_state=None)\n",
        "            contest_folds = list(kf.split(contest_names))\n",
        "\n",
        "            for fold, (train_val_idx, test_idx) in enumerate(contest_folds):\n",
        "                print(f\"\\n--- Run {run}, Fold {fold+1}/{n_folds} ---\")\n",
        "                test_contests = contest_names[test_idx]\n",
        "                train_val_contests = contest_names[train_val_idx]\n",
        "\n",
        "                # 80/20 split of the 80% (train_val_contests)\n",
        "                train_contests, val_contests = train_test_split(train_val_contests, test_size=0.2, random_state=None)\n",
        "\n",
        "                train_df = self.df[self.df['contest_name'].isin(train_contests)]\n",
        "                val_df = self.df[self.df['contest_name'].isin(val_contests)]\n",
        "                test_df = self.df[self.df['contest_name'].isin(test_contests)]\n",
        "\n",
        "                # Prepare DataLoaders for MLP\n",
        "                train_dataset = CSVDataset(train_df)\n",
        "                val_dataset = CSVDataset(val_df, smote=False)\n",
        "                train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "                # ---------------------------------\n",
        "                # Train model\n",
        "                # ---------------------------------\n",
        "                if model_type == \"MLP\":\n",
        "                    model = SimpleMLP(\n",
        "                        input_size=11,\n",
        "                        hidden_size=mlp_hidden_size,\n",
        "                        output_size=1\n",
        "                    )\n",
        "                    trainer = pl.Trainer(\n",
        "                        max_epochs=mlp_epochs,\n",
        "                        enable_checkpointing=False,\n",
        "                        logger=False\n",
        "                    )\n",
        "                    trainer.fit(model, train_loader, val_loader)\n",
        "                    model.eval()\n",
        "\n",
        "                else:\n",
        "                    # Sklearn-based model\n",
        "                    model_wrapper = OptimizedModelWrapper(\n",
        "                        model_name=model_type,\n",
        "                        train_X=train_dataset.x,\n",
        "                        train_y=train_dataset.y,\n",
        "                        val_X=val_dataset.x,\n",
        "                        val_y=val_dataset.y\n",
        "                    )\n",
        "                    model = model_wrapper.model  # The optimized sklearn model\n",
        "\n",
        "                # ---------------------------------\n",
        "                # Evaluate model (fold-level)\n",
        "                # ---------------------------------\n",
        "                df_results, accuracy, overall_recall, conf_matrix, fold_metrics_df, fold_metrics_data = self.submission_evaluation(\n",
        "                    model,\n",
        "                    test_df,\n",
        "                    run,\n",
        "                    fold,\n",
        "                    dataset_name,\n",
        "                    model_type,\n",
        "                    save_contest_results=save_contest_results,\n",
        "                    save_false_negatives=save_false_negatives,\n",
        "                    save_all_submissions=save_all_submissions,\n",
        "                    compute_additional_metrics=compute_additional_metrics\n",
        "                )\n",
        "\n",
        "                recall_per_fold.append(overall_recall)\n",
        "\n",
        "                # ---------------------------------\n",
        "                # ACCUMULATE fold-level metrics into run_metrics_dict\n",
        "                # ---------------------------------\n",
        "                if compute_additional_metrics and fold_metrics_data:\n",
        "                    for p in fold_metrics_data.keys():\n",
        "                        run_metrics_dict[p][\"tn\"] += fold_metrics_data[p][\"tn\"]\n",
        "                        run_metrics_dict[p][\"fp\"] += fold_metrics_data[p][\"fp\"]\n",
        "                        run_metrics_dict[p][\"fn\"] += fold_metrics_data[p][\"fn\"]\n",
        "                        run_metrics_dict[p][\"tp\"] += fold_metrics_data[p][\"tp\"]\n",
        "\n",
        "                        run_metrics_dict[p][\"acc_list\"].extend(fold_metrics_data[p][\"acc_list\"])\n",
        "                        run_metrics_dict[p][\"recall_list\"].extend(fold_metrics_data[p][\"recall_list\"])\n",
        "                        run_metrics_dict[p][\"remain_pct_list\"].extend(fold_metrics_data[p][\"remain_pct_list\"])\n",
        "\n",
        "                # Clear GPU cache if using GPU\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # -----------------------------\n",
        "            # After all folds of this run, build final run-level metrics\n",
        "            # -----------------------------\n",
        "            if compute_additional_metrics:\n",
        "                final_rows = []\n",
        "                for p in filter_percentages:\n",
        "                    data_p = run_metrics_dict[p]\n",
        "\n",
        "                    # Sum of confusion matrix terms across folds\n",
        "                    tn = data_p[\"tn\"]\n",
        "                    fp = data_p[\"fp\"]\n",
        "                    fn = data_p[\"fn\"]\n",
        "                    tp = data_p[\"tp\"]\n",
        "                    total = tn + fp + fn + tp\n",
        "\n",
        "                    # Compute Accuracy & Recall from the overall sums\n",
        "                    accuracy_from_sums = (tn + tp) / total if total > 0 else 0.0\n",
        "                    recall_from_sums = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "                    # For Remaining% we can still average across folds\n",
        "                    avg_remain_pct = np.mean(data_p[\"remain_pct_list\"]) if data_p[\"remain_pct_list\"] else 0.0\n",
        "\n",
        "                    row = {\n",
        "                        \"Filter_Percentage\": p,\n",
        "                        \"Remaining_Percentage_Size_of_Submissions\": avg_remain_pct,\n",
        "                        \"Accuracy\": accuracy_from_sums,\n",
        "                        \"Recall\": recall_from_sums,\n",
        "                        \"True_Negatives\": tn,\n",
        "                        \"False_Positives\": fp,\n",
        "                        \"False_Negatives\": fn,\n",
        "                        \"True_Positives\": tp\n",
        "                    }\n",
        "                    final_rows.append(row)\n",
        "\n",
        "                run_metrics_summary_df = pd.DataFrame(final_rows)\n",
        "                print(f\"\\n========== RUN {run} - FINAL ADDITIONAL METRICS ACROSS ALL FOLDS ==========\")\n",
        "                print(run_metrics_summary_df)\n",
        "\n",
        "                # Save this run-level summary to CSV\n",
        "                additional_metrics_csv = os.path.join(\n",
        "                    self.output_path,\n",
        "                    f\"{model_type}_additional_metrics_{dataset_name[:-4]}_run{run}.csv\"\n",
        "                )\n",
        "                run_metrics_summary_df.to_csv(additional_metrics_csv, index=False)\n",
        "                print(f\"Run-level Additional Metrics saved to {additional_metrics_csv}\")\n",
        "\n",
        "            # -----------------------------\n",
        "            # Print average recall for this run (at 50% filter) if desired\n",
        "            # -----------------------------\n",
        "            valid_recalls = [r for r in recall_per_fold if r is not None]\n",
        "            avg_recall_50 = np.mean(valid_recalls) if len(valid_recalls) > 0 else 0.0\n",
        "            print(f\"\\nAverage Recall at 50% filter across all folds (RUN {run}): {avg_recall_50:.4f}\")\n",
        "\n",
        "    def aggregate_average_recall_across_runs(self, model_type, dataset_name, n_runs=5):\n",
        "        \"\"\"\n",
        "        Reads all run-level CSVs for a given model_type (e.g., \"MLP\") and dataset_name,\n",
        "        then computes the average Recall across all runs for each Filter_Percentage.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): The model type (\"MLP\", \"RandomForest\", \"XGBoost\", \"SVM\", \"LogisticRegression\").\n",
        "            dataset_name (str): The dataset filename (used to locate CSVs).\n",
        "            n_runs (int): Number of runs that were performed (defaults to 5).\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame or None:\n",
        "                A DataFrame with two columns: [\"Filter_Percentage\", \"Average_Recall\"],\n",
        "                or None if no CSV files were found.\n",
        "\n",
        "        Example:\n",
        "            avg_recall_df = discriminative_model.aggregate_average_recall_across_runs(\"MLP\", \"28_11.csv\", 5)\n",
        "        \"\"\"\n",
        "        import os\n",
        "        import pandas as pd\n",
        "\n",
        "        dfs = []\n",
        "        # Gather all CSV files for the specified runs\n",
        "        for run in range(1, n_runs + 1):\n",
        "            csv_name = f\"{model_type}_additional_metrics_{dataset_name[:-4]}_run{run}.csv\"\n",
        "            csv_path = os.path.join(self.output_path, csv_name)\n",
        "            if os.path.exists(csv_path):\n",
        "                df_run = pd.read_csv(csv_path)\n",
        "                dfs.append(df_run)\n",
        "            else:\n",
        "                print(f\"Warning: CSV not found for run {run}: {csv_path}\")\n",
        "\n",
        "        if not dfs:\n",
        "            print(\"No CSV files found for the specified model_type and dataset_name.\")\n",
        "            return None\n",
        "\n",
        "        # Concatenate all runs\n",
        "        combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        # Compute average recall for each Filter_Percentage\n",
        "        avg_recall_df = (\n",
        "            combined_df\n",
        "            .groupby(\"Filter_Percentage\", as_index=False)[\"Recall\"]\n",
        "            .mean()\n",
        "            .rename(columns={\"Recall\": \"Average_Recall\"})\n",
        "        )\n",
        "\n",
        "        # Print and return the resulting DataFrame\n",
        "        print(f\"\\n=== Average Recall Across {n_runs} Runs for {model_type} ===\")\n",
        "        print(avg_recall_df)\n",
        "        return avg_recall_df\n",
        "\n"
      ],
      "metadata": {
        "id": "f_SKdd9hQmzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ModelComparisonPlotter\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class ModelComparisonPlotter:\n",
        "    \"\"\"\n",
        "    Class Name: ModelComparisonPlotter\n",
        "\n",
        "    Purpose:\n",
        "        - Compare recall performance of multiple models at various filter percentages.\n",
        "        - Aggregate average recall (and potentially other metrics) for each model and plot them on a single chart.\n",
        "\n",
        "    Responsibilities:\n",
        "        - Scan CSV files produced by Evaluator (e.g., \"MLP_additional_metrics_28_11_run1.csv\").\n",
        "        - Aggregate average recall from each model across multiple runs.\n",
        "        - Combine these into a single DataFrame (filter_percentage vs. recall per model).\n",
        "        - Plot these recalls on a single figure for visual comparison.\n",
        "\n",
        "    Example Usage:\n",
        "        plotter = ModelComparisonPlotter(output_path, dataset_name=\"28_11.csv\", n_runs=5)\n",
        "        model_list = [\"MLP\", \"RandomForest\", \"XGBoost\", \"SVM\", \"LogisticRegression\"]\n",
        "        combined_df = plotter.gather_average_recall_all_models(model_list)\n",
        "        plotter.save_combined_averages(combined_df, \"all_models_avg_recall.csv\")\n",
        "        plotter.plot_model_recall_comparison(combined_df, save_path=\"model_recall_plot.png\")\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_path, dataset_name, n_runs=5):\n",
        "        \"\"\"\n",
        "        Constructor for ModelComparisonPlotter.\n",
        "\n",
        "        Args:\n",
        "            output_path (str): The directory where the run CSVs are located.\n",
        "            dataset_name (str): The dataset file name (used to locate CSVs).\n",
        "            n_runs (int): Number of runs performed for each model.\n",
        "        \"\"\"\n",
        "        self.output_path = output_path\n",
        "        self.dataset_name = dataset_name\n",
        "        self.n_runs = n_runs\n",
        "\n",
        "    def gather_average_recall_all_models(self, model_types):\n",
        "        \"\"\"\n",
        "        Gathers the average recall for each model across multiple runs,\n",
        "        then combines them into a single DataFrame with columns [Filter_Percentage, <Model1>, <Model2>, ...].\n",
        "\n",
        "        Args:\n",
        "            model_types (list of str): A list of model type strings, e.g. [\"MLP\", \"RandomForest\", \"XGBoost\"].\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Combined DataFrame with columns:\n",
        "                - Filter_Percentage\n",
        "                - <ModelName1>\n",
        "                - <ModelName2>\n",
        "                - ...\n",
        "        \"\"\"\n",
        "        filter_percentage_col = \"Filter_Percentage\"\n",
        "        # We'll store each model's average recall data in a dict: { model_name: DataFrame_of_recall }\n",
        "        model_recall_frames = {}\n",
        "\n",
        "        for model_name in model_types:\n",
        "            # For each model, read all runs and compute average recall at each filter percentage\n",
        "            dfs = []\n",
        "            for run in range(1, self.n_runs + 1):\n",
        "                csv_filename = f\"{model_name}_additional_metrics_{self.dataset_name[:-4]}_run{run}.csv\"\n",
        "                csv_path = os.path.join(self.output_path, csv_filename)\n",
        "                if os.path.exists(csv_path):\n",
        "                    df_run = pd.read_csv(csv_path)\n",
        "                    dfs.append(df_run)\n",
        "                else:\n",
        "                    print(f\"Warning: CSV not found for run {run}: {csv_path}\")\n",
        "\n",
        "            if not dfs:\n",
        "                print(f\"No data found for model: {model_name}\")\n",
        "                continue\n",
        "\n",
        "            # Concatenate all runs for this model\n",
        "            combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "            # Group by Filter_Percentage and compute the mean of \"Recall\"\n",
        "            avg_recall_df = (\n",
        "                combined_df\n",
        "                .groupby(filter_percentage_col, as_index=False)[\"Recall\"]\n",
        "                .mean()\n",
        "                .rename(columns={\"Recall\": model_name})  # rename column to the model name\n",
        "            )\n",
        "            # Store it\n",
        "            model_recall_frames[model_name] = avg_recall_df\n",
        "\n",
        "        if not model_recall_frames:\n",
        "            print(\"No models had valid data; returning empty DataFrame.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Now we need to merge all these DataFrames on Filter_Percentage\n",
        "        # Start with one of them arbitrarily:\n",
        "        all_models_merged = None\n",
        "        for i, (model_name, df_model) in enumerate(model_recall_frames.items()):\n",
        "            if i == 0:\n",
        "                all_models_merged = df_model\n",
        "            else:\n",
        "                all_models_merged = pd.merge(all_models_merged, df_model, on=filter_percentage_col, how=\"outer\")\n",
        "\n",
        "        # Sort by Filter_Percentage\n",
        "        all_models_merged = all_models_merged.sort_values(by=[filter_percentage_col]).reset_index(drop=True)\n",
        "        return all_models_merged\n",
        "\n",
        "    def save_combined_averages(self, combined_df, filename=\"all_models_avg_recall.csv\"):\n",
        "        \"\"\"\n",
        "        Saves the combined average recall DataFrame to a CSV.\n",
        "\n",
        "        Args:\n",
        "            combined_df (pd.DataFrame): The DataFrame generated by gather_average_recall_all_models().\n",
        "            filename (str): Name of the output CSV file.\n",
        "        \"\"\"\n",
        "        output_csv = os.path.join(self.output_path, filename)\n",
        "        combined_df.to_csv(output_csv, index=False)\n",
        "        print(f\"Combined averages saved to {output_csv}\")\n",
        "\n",
        "    def plot_model_recall_comparison(self, combined_df, save_path=None):\n",
        "        \"\"\"\n",
        "        Plots the average recall comparison at different filter percentages for each model.\n",
        "\n",
        "        Args:\n",
        "            combined_df (pd.DataFrame): DataFrame from gather_average_recall_all_models().\n",
        "                                         Columns: [Filter_Percentage, MLP, RandomForest, ...].\n",
        "            save_path (str, optional): If provided, saves the figure to this path.\n",
        "                                       Otherwise, shows the plot interactively.\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # We'll assume the first column is Filter_Percentage, the rest are model columns\n",
        "        if combined_df.empty:\n",
        "            print(\"Warning: combined_df is empty. Nothing to plot.\")\n",
        "            return\n",
        "\n",
        "        # X-values\n",
        "        filter_percentages = combined_df[\"Filter_Percentage\"].values\n",
        "        # Model columns\n",
        "        model_columns = [c for c in combined_df.columns if c != \"Filter_Percentage\"]\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        for model_col in model_columns:\n",
        "            plt.plot(filter_percentages, combined_df[model_col], marker='o', label=model_col)\n",
        "\n",
        "        plt.title(\"Model Recall Comparison at Different Filtering Percentages\")\n",
        "        plt.xlabel(\"Filtering Percentage (%)\")\n",
        "        plt.ylabel(\"Average Recall\")\n",
        "        plt.xticks(filter_percentages)\n",
        "        plt.ylim([0, 1])  # typical recall range\n",
        "        plt.legend(loc=\"lower left\")\n",
        "        plt.grid(True)\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            print(f\"Plot saved to: {save_path}\")\n",
        "        else:\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "oNjA6uOfG83p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "drive_mgr = DriveManager()\n",
        "drive_mgr.mount_drive()\n",
        "\n",
        "# Load dataset\n",
        "input_path = '/content/drive/MyDrive/Master_Thesis/ML_Input/'\n",
        "dataset_name = 'final_dataset.csv'\n",
        "full_dataset_path = os.path.join(input_path, dataset_name)\n",
        "df = pd.read_csv(full_dataset_path)"
      ],
      "metadata": {
        "id": "e2o9vLdFZSzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DiscriminativeModel instance\n",
        "output_path = '/content/drive/MyDrive/Master_Thesis/MLP_Output/'\n",
        "discriminative_model = DiscriminativeModel(df, output_path=output_path)\n",
        "\n",
        "model_type = \"MLP\"  # choose: MLP, RandomForest, XGBoost, SVM, LogisticRegression\n",
        "amount_runs = 5\n",
        "amount_folds = 5\n",
        "\n",
        "# Run evaluation with desired parameters\n",
        "discriminative_model.run_evaluation(\n",
        "    n_runs=amount_runs,\n",
        "    n_folds=amount_folds,\n",
        "    model_type=model_type,\n",
        "    dataset_name=dataset_name,\n",
        "    save_contest_results=True,\n",
        "    save_false_negatives=False,\n",
        "    save_all_submissions=True,\n",
        "    compute_additional_metrics=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "GUHttlvPYf1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotter = ModelComparisonPlotter(\n",
        "    output_path=\"/content/drive/MyDrive/Master_Thesis/MLP_Output/\",\n",
        "    dataset_name=\"28_11.csv\",\n",
        "    n_runs=5\n",
        ")\n",
        "\n",
        "\n",
        "models_to_compare = [\"MLP\", \"RandomForest\", \"XGBoost\", \"SVM\", \"LogisticRegression\"]\n",
        "combined_df = plotter.gather_average_recall_all_models(models_to_compare)\n",
        "\n",
        "\n",
        "plotter.save_combined_averages(combined_df, \"all_models_avg_recall.csv\")\n",
        "\n",
        "\n",
        "plot_path = \"/content/drive/MyDrive/Master_Thesis/MLP_Output/model_recall_comparison.png\"\n",
        "plotter.plot_model_recall_comparison(combined_df, save_path=plot_path)\n"
      ],
      "metadata": {
        "id": "VFpluLWmHRce"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}